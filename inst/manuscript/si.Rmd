---
title: "Supporting Information: Improving cropland maps through tight integration of human and machine intelligence"

output:
  pdf_document:
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_tex: yes
    number_sections: yes
    template: manuscript.latex
    includes:
      in_header: header.tex

  html_document: null
  
  word_document: null

documentclass: article
classoption: a4paper
capsize: normalsize
fontsize: 11pt
geometry: margin=1in
linenumbers: false
spacing: doublespacing
# abstract: The abstract can go either here or below
keywords: rmarkdown, reproducible science
bibliography: 
  - references.bib
  - knitcitations.bib
csl: ecology.csl

---

<!-- Provides custom captions for figures in supplement -->
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }
     
```{r setup, include=FALSE, cache=FALSE, message = FALSE}
library(knitr)
library(citr)
library(tidyverse)
library(sf)
library(activemapper)

#opts_knit$set(root.dir=normalizePath('../'))

### Chunk options: see http://yihui.name/knitr/options/ ###

## Text results
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)

## Code decoration
opts_chunk$set(tidy = TRUE, comment = NA, highlight = TRUE)

## Cache
opts_chunk$set(cache = 2, cache.path = "output/cache/")

## Plots
opts_chunk$set(fig.path = "output/figures/")


```


```{r knitcitations, echo=FALSE, cache = FALSE}
library(knitcitations)
cleanbib()   
cite_options(citation_format = "pandoc")
```

\beginsupplement
\singlespace

\bleft


# Methods
## Assessing quality of composites
The rubric presented in Table \ref{tab:imqcriteria} was used to assess the quality of the seasonal image composites. The imagery was evaluated by examining their Raster Foundry overlays within a `labeller` instance set up for the purpose. 

```{r imqcriteria, echo=FALSE, message=FALSE}
library(kableExtra)
image_qual_criteria <- tibble(
  `Quality dimension` = c(
    "Percent affected by residual cloud", "Percent affected by cloud shadow", 
    "Number of visible scene boundaries", "Percent blurred"
  ),
  `3 pts` = c("<1%", "<1%", "None", "None"), 
  `2 pts` = c("1-5%", "1-5%", "1", "<20%"), 
  `1 pts` = c("5-10%", "5-10%", "2-3", "20-50%"),
  `0 pts` = c(">10%", ">10%", ">3", ">50%")
) 
cap <- glue::glue("Four dimensions used to assess the assess the quality
                  of the temporally composited image tiles, including the
                  criteria used to award points for scoring each dimension.")
kable(image_qual_criteria, format = "latex", booktabs = "T", caption = cap) %>% 
  kable_styling(full_width = TRUE, latex_options = c("hold_position")) %>% 
  column_spec(1, width = "8cm")
```


## Mapping platform

### Digitizing tools
To minimize the risk of topological errors, *mapper*'s polygon digitizing tools prevent drawing that results in self-intersections and overlaps between adjacent polygons. Upon submission, the PostGIS [`ST_MakeValid`](https://www.postgis.net/docs/ST_MakeValid.html) function is applied to each polygon's geometry to clean remaining topological errors upon insertion into the database. 

### Consensus labelling
As described in the main text, the formula used for creating a consensus label is: 

\begin{equation} \label{eq:main}
\mathrm{P(\theta|D)=\sum_{i=1}^{n}P(W_i|D)P(\theta|D, W_i)}
\end{equation}

Where $\theta$ represents the true cover type of a pixel (field or not field), *D* is the worker's label of that field, and *W$_i$* is an individual worker.  Looking in greater details at this equation, the first half of the righthand side of the equation, P(W$_i$|D), is the "prior" for worker *i* for the current site based on their history of scores from prior accuracy assessment assignments. The second term, P($\theta$|D, W$_i$), is the probability that the actual class of the pixel in the current assignment is the class that worker *i*'s says that it is, which is either 0 or 1. There are four possible values for this second term:

\begin{equation} \label{eq:tp}
P(\theta = field|D_i = field) = 1
\end{equation}
\begin{equation} \label{eq:fp}
P(\theta = no field|D_i = field) = 0
\end{equation}
\begin{equation} \label{eq:tn}
P(\theta = no field|D_i = no field) = 1
\end{equation}
\begin{equation} \label{eq:fn}
P(\theta = field|D_i = no field) = 0
\end{equation}

Where equations \ref{eq:tp} and \ref{eq:tn} represent true positives and negatives, respectively, and equation \ref{eq:fp} is a false positive, and equation \ref{eq:fn} is a false negative.  

Coming back to the first term, the calculation of prior probability can be re-expressed as:

\begin{equation} \label{eq:prior}
\mathrm{P(W_i|D) \approx P(D|W_i)P(W_i)}
\end{equation}

Where:

\begin{equation} 
\mathrm{P(D|W_i) \propto exp\left(-\frac{1}{2}BIC_i\right)}
\end{equation}

With BIC being the Bayesian information criterion:

\begin{equation}
\mathrm{BIC = ln(n)k - 2ln(\hat{L})}
\end{equation}


In which *n* is the sample size, *k* is the number of parameters to estimate, and $\hat{L}$ is the maximum likeihood function. In this case, we are only interested in one parameter (the label that maximizes the likelihood function), thus the BIC becomes:

\begin{equation}
\mathrm{BIC \approx -2ln(\hat{L}) = 2ln(p(D|\hat{\theta}, W))}
\end{equation}

After rearranging, we have:

\begin{equation}
\mathrm{P(D|W_i) \propto p(D|\hat{\theta}, W_i))}
\end{equation}

Which is the worker maximum likelihood, which can be computed as:

\begin{equation} \label{eq:pacc1}
\mathrm{P(\theta = field|\hat{\theta}, M_I) = P(D = field|\theta = field, M_I) = \frac{1}{m}\left(\sum_{j}^{m}\frac{tp_j}{tp_j + fn_j} \right)}
\end{equation}

\begin{equation} \label{eq:pacc2}
\mathrm{P(\theta = no field|\hat{\theta}, M_I) = P(D = no field|\theta = no field, M_I) = \frac{1}{m}\left(\sum_{j}^{m}\frac{tn_j}{tn_j + fp_j} \right)}
\end{equation}

Equations \ref{eq:pacc1} and \ref{eq:pacc2} are producer's accuracies, thus the maximum worker likelihood is equivalent to the worker's average producer's accuracy. 

<!-- \ref{eq:prior} represents "mapper likelihood" for the current map: -->

The other component of equation \ref{eq:prior}, P(W$_i$), is the worker's average score over *m* accuracy assessment assignment:  
\begin{equation} 
\mathrm{P(W_i) \propto \frac{1}{m}\sum_{j=1}^{m}score_j}
\end{equation}

Thus equation \ref{eq:prior} uses two measures of worker accuracy, 1) their overall average accuracy score multipled by 2) their average producer's accuracy to create a *weight* for their individual maps for the given site. Equation \ref{eq:main} becomes:

\begin{equation} 
\mathrm{P(\theta|D)=\frac{\sum_{i=1}^{n}weight_iP(\theta|D, W_i)}{\sum_{i=1}^{n}weight_i}}
\end{equation}

With $\mathrm{P(\theta|D, W_i)}$ being either 0 or 1. In labelling, if the consensus result for a pixel is: $\mathrm{P(\theta = field|D)}$ > 0.5, then we assign that pixel to the field category, otherwise to the no field category. 

After creating the consensus label, the degree of confidence in the resulting label value is measured by Bayes Risk:

\begin{equation} \label{eq:pixelrisk}
\mathrm{r=C(1 - L) + (1 - C)L}
\end{equation}

Where *C* is the consensus probability that a given pixel is a field ($\mathrm{P(\theta = field|D)}$), and *L* is the consensus label (i.e. non-field if *C* < 0.5, field if *C* > 0.5) for that pixel. The risk values across the entire sample site can be processed in two ways to provide useful information about the confidence in the consensus label for that site. The first is a simple average of all risk values in the site, where the slope of the risk varies depending on whether *L* is a field or not a field (Figure \ref{fig:riskcurve}). The closer to 0 the lower the risk that the *L* is mislabelled, while values approaching 1 indicate increasing risk of mislabelling. A second approach is to calculate the proportion of pixels having risk values that exceed a certain threshold. 


```{r riskcurve, echo = FALSE, fig.align='center', out.width="70%", message=FALSE, warning=FALSE, fig.cap="Bayesian risk values (Y-axis) for consensus values (X axis) ranging from 0 to 1 (0 indicates no consensus that a pixel falls into the field class, 1 means complete consensus) for field and non-field consensus labels."}
knitr::include_graphics('figures/si_label_risk.png')
```

#### Example

To provide an example of this approach in practice, we'll imagine two workers A and B, each with the following histories:

```{r, echo = FALSE}
library(magrittr)
sc <- tibble::tibble(Worker = LETTERS[1:2], `Prod. Acc. (Field)` = c(0.8, 0.62),
                     `Prod. Acc (no field)` = c(0.81, 0.61), 
                     Score = c(0.75, 0.60))
wtA <- sc[1, ]$Score * sc[1, ]$`Prod. Acc. (Field)`
wtB <- sc[2, ]$Score * sc[2, ]$`Prod. Acc (no field)`

# sc
knitr::kable(sc, align = c("r", "c", "c"), format = "pandoc")
```

In this scenario, worker A thinks that the given pixel falls within a field, and worker B thinks it is not a field.  First, we calculate the weights for each worker:

\clearpage
$$
Weight_A = score_A * PA_A(field) = P(W_A)P(D=field|W_A) = `r sc[1, 2]` * `r sc[1, 4]` = `r wtA`
$$



$$
Weight_B = score_B * PA_B(field) = P(W_B)P(D=no field|W_B) = `r sc[2, 3]` * `r sc[2, 4]` = `r wtB`
$$

And then we plug these weights into the full equation:

$$
\mathrm{P(\theta|D)=\frac{\sum_{i=1}^{n}weight_iP(L = field|D, W_i)}{\sum_{i=1}^{n}weight_i}} = \frac{`r wtA` * 1 + `r wtB` * 0}{`r wtA` + `r wtB`} = `r round((wtA * 1 + wtB * 0) / (wtA + wtB), 3)`
$$

Since `r rj <- round((wtA * 1 + wtB * 0) / (wtA + wtB), 3); rj` > 0.5, we label the particular pixel a field.  

Using equation \ref{eq:pixelrisk}, the corresponding risk associated with this particular pixel's label is thus `r rj * (1 - 1) + (1 - rj) * 1`. 





### Accuracy assessment
We designed and implemented a map accuracy assessment protocol following procedures summarized by @StehmanKeyissuesrigorous2019. This entailed the creation of a map reference sample, which first entailed designing a sample, and then designing how the sample response would be collected.

#### Map reference sample design
A reference sample should ideally have some randomization involved in the sample selection protocol, and the probabilities for a sample being included should be provided.  There are multiple sample designs that can be chosen, but we employed a stratified sampling design, using the segmented field boundaries to define the strata for field/non-field. To create the sample, we first extracted the centroids for each of the sample grid cells in Ghana. We then intersected the centroid points with the field segments, assigned a class of 1 (cropland) where points intersected a field, and 0 where they didn't (non-cropland). We then removed from this set of points all those that corresponded to model training, validation, or training reference sites. We then extracted a random sample from both the cropland and non-cropland points. To determine the sample size of each, we specified a desired confidence interval using the following formula [@StehmanKeyissuesrigorous2019]:

$$n = \frac{z^2p(1-p)}{d^2}$$
        
Where p is the estimated probability (or mapped class accuracy), and d is the size of the margin of error (1/2 the confidence interval). We selected a d value of 0.03 and assumed that the user's accuracy of the field class would be 0.75 and that of the non-cropland class would be 0.8, returning sample sizes of 800 and 683, respectively. The distribution of the resulting map reference sample is shown in Figure \ref{fig:refsample}.

```{r refsample, echo = FALSE, fig.align='center', out.width="80%", message=FALSE, warning=FALSE, fig.cap="Distribution of the selected map reference sample for the cropland and non-cropland class. "}
knitr::include_graphics('figures/si_map_reference_sample.png')
```

#### Response design

##### Stage 1
We collected the map reference sample on a separate instance of `labeller` set up for the purpose (`validator.crowdmapper.org`). For the sampling unit, we selected a rectangular polygon of ~0.1 ha (0.0002866$^\circ$ resolution), centered on the centroid of each grid cell selected for map reference sample.  Four classes were established for the validation: **cropland**, **non-cropland**, **uncertain but likely cropland**, and **uncertain but likely non-cropland**. The latter two classes were designed to capture information related to swidden dynamics, following the rationale that uncertainty and time since last cropping are likely to be positively correlated. This uncertainty also captures information about the inherent difficulty of the mapping task. Samples were collected by visually interpreting the overlays of PlanetScope composites presented in `labeller`, following the same interpretation protocols used by the labelling team, with the exception that the polygons placed were square and of 0.0002866$^\circ$ resolution. 

The map reference sample was first placed and defined in an initial stage. In this stage, the class of the initial square polygon was determined, and then the polygon was adjusted if it overlapped more than one class. In sites where the initial map reference grid fell on one of the two uncertain classes, a second polygon was placed in the nearest location where a definite cropland or non-cropland interpretation could be made, with the choice determined by whether the uncertain class was more likely cropland or not cropland. The uncertain class was only used for providing insight into fallow cycles, while the second, more certain polygon was used in a second stage of the map reference sample, which was given to multiple independent interpreters (see below). 

The following set of rules were followed in this first, sample placing phase (undertaken by Lyndon Estes):

1. When determining the class corresponding with the initial location of the target grid, IF:
  - More than half of the target falls within what appears to be a clear arable crop field, then assign it to **cropland**. 
  - More than half falls in what is clearly not a field, then assign it to **non-cropland**. 
  - More than half falls in a location where it is harder to tell whether it is cropland or non-cropland, determine whether it is more likely a crop field or not a crop field, and then assign either **uncertain but likely cropland**, or **uncertain but likely non-cropland**.
  
2. After determining the class, IF:
  a. The target polygon is contained entirely within a single clear class AND
      - The class is either **cropland** or **non-cropland** THEN:
        - Digitize a point within the center of the target box, assign appropriate class, and complete the assignment
      - The class belongs to one of the two **uncertain** classes THEN:
      
        - Digitize a square aligned exactly with the initial target polygon, and choose the class **cropland** or **non-cropland** (following the most likely interpretation) and move the new polygon to the nearest location where it can be contained entirely within that clear **cropland** or **non-cropland** class;
        
        - Digitize a point in the middle of the target polygon, and assign it the appropriate label: **cropland** or **non-cropland**. 
        
  b. The target polygon partially overlaps a second class AND:
      - That class is either **cropland** or **non-cropland** THEN:
      
        - Digitize a square aligned exactly with the target polygon, assign the appropriate class, and move the box to the nearest location (shortest possible distance) where it can be contained entirely within that same class.  
  
      - That class is **uncertain** THEN:
    
        - Digitize a square aligned exactly with the target polygon, assign the appropriate **uncertain** class, then move the box to the nearest location (shortest possible distance) where it can be contained entirely within that same **uncertain** class.
        - Digitize a second square aligned exactly with the initial target polygon, and choose the class **cropland** or **non-cropland** (following the most likely interpretation) and move the new polygon to the nearest location where it can be contained entirely within that clear **cropland** or **non-cropland** class.
  

We then 
  - Definition of spatial assessment
  - Definition of classes
  - Reference source
  - Specific information collected from each source
  - Rules for reference class labels
  - How agreement between map and reference classes is defined

##### Stage 2



\clearpage
# Results
## Image quality
```{r, echo=FALSE}
data("image_quality")
wqual_diff <- image_quality$img %>% filter(kml_type == "F") %>%
  group_by(name, season, worker_id) %>%
  summarize(tile = min(tile), score = sum(score) / 12) %>% 
  ungroup() %>% 
  pivot_wider(names_from = worker_id, values_from = score,
              names_prefix = "w") %>%
  select(tile, season, w92, w99) %>% 
  mutate(d = w92 - w99, 
         absd = abs(d))
```

The image quality assessment was conducted by two separate observers (L. Song and Q. Zhang). Each observer assessed the composites for each season at all 50 of the randomly selected tiles (see main text) following the defined criteria (Table \ref{tab:imqcriteria}). To calculate the score for a given tile for each season, we summed the ranking across the four categories for each observer, rescaled the values, and then calculated the mean tile score across both observers by season. The mean difference between the two observers was `r d <- round(mean(wqual_diff$d), 3); d` (sd = `r absd <- round(sd(wqual_diff$d), 3); absd`) and the mean absolute difference was `r round(mean(wqual_diff$absd), 3)` (sd = `r round(sd(wqual_diff$absd), 3)`), thus one observer scored tiles about `r abs(round(d * 100))` percent lower than the other observer, on average.

## Active learning process
### Labelling
The distributions of the initial 500 sites used to train the RandomForest model for each AOI are shown in Figure \ref{fig:trainval}. In AOI 3, the initial active learning cycle resulted in low accuracy because the northern part of the AOI shows low contrast between fields and the surrounding vegetation in the dry season. Training the model with the initial 500 samples resulted in large commission errors in this part of the AOI, thus we ran a second active learning cycle that began with an inital random draw of 300 training sites confined to this AOI (blue points in Figure \ref{fig:trainval}A)
```{r trainval, echo = FALSE, out.width="80%", fig.cap="The distribution of A) initial randomly selected training sites, including 300 points selected within AOI 3 to retrain a second active learning run for this region, as well as the locations of training reference sites, and B) validation points and training sites selected during each active learning iteration.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_training_validation_pts.png')
```

The distribution of data collection effort and scores from training reference sites by labelling team (Figure \ref{fig:trainqual}). These results include those from repeat labelling of the initial training in Cluster 2. This was done because we discovered a small spatial offset in the original image composites, which we corrected by reprocessing the images. We replaced the image overlays in the instance for Cluster 2 \ref{fig:trainval}A), and the labelling team mapped these sites a second time during the production run in late 2019. The reprocessed labels were used to initially train the model for AOIs 9, 12, and 15.

```{r trainqual, echo = FALSE, out.width="60%", fig.cap="The A) distributions of scores at training reference sites for each labeller (means indicated by X in boxplots), and B) the number of training and training reference sites completed by each labeller. Labellers' identities are anonymized.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_training_qual_n.png')
```

The average Bayes Risk of each training and validation site is shown in Figure \ref{fig:labelrisk}, as well as the distribution of risk values per AOI and the initial training clusters (Figure \ref{fig:labelriskhist}). The three initial clusters include the second mapping of Cluster 2. 

```{r labelrisk, echo = FALSE, out.width="80%", fig.cap="The average Bayes Risk of each training and validation site in Ghana.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_label_risk_map.png')
```


```{r labelriskhist, echo = FALSE, out.width="45%", fig.cap="The distribution of Bayes Risk values for training (left column) and validation (right column) sites in each AOI, with the average value indicated by vertical lines. The first three rows (panel titles beginning with 'Cl') indicate distributions of Bayes Risk in the initial training clusters, including the first ('Cl2a') and second ('Cl2b') mappings of Cluster 2.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_label_risk_hists.png')
```


### Validation
The differences in accuracy, AUC, and F1 between the active learning process and the random retraining at each iteration for AOIs 1, 8, and 15 (Figure \ref{fig:randomvactive}). Small differences due to random variations in the RandomForest models are the reason for the non-zero differences at iteration 0, when both models were trained with the same level set.   
```{r randomvactive, echo = FALSE, out.width="90%", fig.cap="The percent difference in performance metrics per iteration for AOIs 1, 8, and 15 (grey lines with numbers indicating AOI; the black line indicates average difference across the three AOIs) when comparing models trained using active learning versus those trained using randomly selected sites. Postive percentages indicate superior performance by active learning, negative percentages the inverse.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_random_vs_active.png')
```

### The impact of training data error
Probability images resulting from Random Forest models trained with labels generated by 1) consensus and the 2) most and 3) least accurate worker to digitize each training site are illustrated in Figure \ref{fig:labelstrategy}. These images were created for a single tile in AOI 1.    

```{r labelstrategy, echo = FALSE, out.width="100%", fig.cap="Cropland probability images produced by RandomForest models trained with A) consensus labels, B) the most accurate worker's labels, and C) the least accurate worker's labels.", fig.align='center', message=FALSE}
knitr::include_graphics('figures/si_label_strategy_probs.png')
```


## Map accuracy 
### Categorical
[To fill]

### Field area and number
To mean, median, and distributions of the average area of segmented field boundaries over the 100 validation sites in each AOI are compared to the areas of the polygons digitized by the most accurate labeller over the same sites in Figure \ref{fig:areavalidation}. The same statistics for average number of segments versus average number of labelled polygons across validation sites in each AOI are shown in Figure \ref{fig:numbervalidation}.      
```{r areavalidation, echo = FALSE, out.width="100%", fig.cap="The distributions of the average areas (in hectares) of segmented field boundaries (shown in blue) at the 100 validation sites per AOI, compared with the average areas of field boundaries digitized (shown in red) by the most accurate worker to label each site. Vertical lines indicate the mean and median of each distribution.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_validation_stats_fha.png')
```

```{r numbervalidation, echo = FALSE, out.width="100%", fig.cap="The distributions of the average number of segmented field boundaries (shown in blue) at the 100 validation sites per AOI, compared with the average number of digitized polygons (shown in red) by the most accurate worker to label each site. The mean and median of each distribution is shown. Vertical lines indicate the mean and median of each distribution.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_validation_stats_fn.png')
```





\clearpage
# References
\singlespace


<div id = "refs"></div>


\eleft

\clearpage





```{r sessioninfo, echo = FALSE, eval = FALSE}
# set eval = FALSE if you don't want this info (useful for reproducibility) to appear 
sessionInfo()
```
