---
title: "Supporting Information: Improving cropland maps through tight integration of human and machine intelligence"

output:
  pdf_document:
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_tex: yes
    number_sections: yes
    template: manuscript.latex
    includes:
      in_header: header.tex

  html_document: null
  
  word_document: null

documentclass: article
classoption: a4paper
capsize: normalsize
fontsize: 11pt
geometry: margin=1in
linenumbers: false
spacing: doublespacing
footerdate: yes
# abstract: The abstract can go either here or below
keywords: rmarkdown, reproducible science
bibliography: 
  - references.bib
  - knitcitations.bib
csl: ecology.csl

---

<!-- Provides custom captions for figures in supplement -->
\newcommand{\beginsupplement}{%
        \setcounter{table}{0}
        \renewcommand{\thetable}{S\arabic{table}}%
        \setcounter{figure}{0}
        \renewcommand{\thefigure}{S\arabic{figure}}%
     }

```{r setup, include=FALSE, cache=FALSE, message = FALSE}
library(knitr)
library(citr)
library(tidyverse)
library(sf)
library(activemapper)

#opts_knit$set(root.dir=normalizePath('../'))

### Chunk options: see http://yihui.name/knitr/options/ ###

## Text results
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)

## Code decoration
opts_chunk$set(tidy = TRUE, comment = NA, highlight = TRUE)

## Cache
opts_chunk$set(cache = 2, cache.path = "output/cache/")

## Plots
opts_chunk$set(fig.path = "output/figures/")


```


```{r knitcitations, echo=FALSE, cache = FALSE}
library(knitcitations)
cleanbib()   
cite_options(citation_format = "pandoc")
```

\beginsupplement
\singlespace

\bleft


# Methods
## Assessing quality of composites
The rubric presented in Table 1 was used to assess the quality of the seasonal image composites. The imagery was evaluated by examining their Raster Foundry overlays within a `labeller` instance set up for the purpose. 

```{r, echo=FALSE, message=FALSE}
library(kableExtra)
image_qual_criteria <- tibble(
  `Quality dimension` = c(
    "Percent affected by residual cloud", "Percent affected by cloud shadow", 
    "Number of visible scene boundaries", "Percent blurred"
  ),
  `3 pts` = c("<1%", "<1%", "None", "None"), 
  `2 pts` = c("1-5%", "1-5%", "1", "<20%"), 
  `1 pts` = c("5-10%", "5-10%", "2-3", "20-50%"),
  `0 pts` = c(">10%", ">10%", ">3", ">50%")
) 
cap <- glue::glue("Four dimensions used to assess the assess the quality
                  of the temporally composited image tiles, including the
                  criteria used to award points for scoring each dimension.")
kable(image_qual_criteria, format = "latex", booktabs = "T", caption = cap) %>% 
  kable_styling(full_width = TRUE, latex_options = c("hold_position")) %>% 
  column_spec(1, width = "8cm")
```


## Mapping platform

### Digitizing tools
To minimize the risk of topological errors, *mapper*'s polygon digitizing tools prevent drawing that results in self-intersections and overlaps between adjacent polygons. Upon submission, the PostGIS [`ST_MakeValid`](https://www.postgis.net/docs/ST_MakeValid.html) function is applied to each polygon's geometry to clean remaining topological errors upon insertion into the database. 

### Consensus labelling
As described in the main text, the formula used for creating a consensus label is: 

\begin{equation} \label{eq:main}
\mathrm{P(\theta|D)=\sum_{i=1}^{n}P(W_i|D)P(\theta|D, W_i)}
\end{equation}

Where $\theta$ represents the true cover type of a pixel (field or not field), *D* is the worker's label of that field, and *W$_i$* is an individual worker.  Looking in greater details at this equation, the first half of the righthand side of the equation, P(W$_i$|D), is the "prior" for worker *i* for the current site based on their history of scores from prior accuracy assessment assignments. The second term, P($\theta$|D, W$_i$), is the probability that the actual class of the pixel in the current assignment is the class that worker *i*'s says that it is, which is either 0 or 1. There are four possible values for this second term:

\begin{equation} \label{eq:tp}
P(\theta = field|D_i = field) = 1
\end{equation}
\begin{equation} \label{eq:fp}
P(\theta = no field|D_i = field) = 0
\end{equation}
\begin{equation} \label{eq:tn}
P(\theta = no field|D_i = no field) = 1
\end{equation}
\begin{equation} \label{eq:fn}
P(\theta = field|D_i = no field) = 0
\end{equation}

Where equations \ref{eq:tp} and \ref{eq:tn} represent true positives and negatives, respectively, and equation \ref{eq:fp} is a false positive, and equation \ref{eq:fn} is a false negative.  

Coming back to the first term, the calculation of prior probability can be re-expressed as:

\begin{equation} \label{eq:prior}
\mathrm{P(W_i|D) \approx P(D|W_i)P(W_i)}
\end{equation}

Where:

\begin{equation} 
\mathrm{P(D|W_i) \propto exp\left(-\frac{1}{2}BIC_i\right)}
\end{equation}

With BIC being the Bayesian information criterion:

\begin{equation}
\mathrm{BIC = ln(n)k - 2ln(\hat{L})}
\end{equation}


In which *n* is the sample size, *k* is the number of parameters to estimate, and $\hat{L}$ is the maximum likeihood function. In this case, we are only interested in one parameter (the label that maximizes the likelihood function), thus the BIC becomes:

\begin{equation}
\mathrm{BIC \approx -2ln(\hat{L}) = 2ln(p(D|\hat{\theta}, W))}
\end{equation}

After rearranging, we have:

\begin{equation}
\mathrm{P(D|W_i) \propto p(D|\hat{\theta}, W_i))}
\end{equation}

Which is the worker maximum likelihood, which can be computed as:

\begin{equation} \label{eq:pacc1}
\mathrm{P(\theta = field|\hat{\theta}, M_I) = P(D = field|\theta = field, M_I) = \frac{1}{m}\left(\sum_{j}^{m}\frac{tp_j}{tp_j + fn_j} \right)}
\end{equation}

\begin{equation} \label{eq:pacc2}
\mathrm{P(\theta = no field|\hat{\theta}, M_I) = P(D = no field|\theta = no field, M_I) = \frac{1}{m}\left(\sum_{j}^{m}\frac{tn_j}{tn_j + fp_j} \right)}
\end{equation}

Equations \ref{eq:pacc1} and \ref{eq:pacc2} are producer's accuracies, thus the maximum worker likelihood is equivalent to the worker's average producer's accuracy. 

<!-- \ref{eq:prior} represents "mapper likelihood" for the current map: -->

The other component of equation \ref{eq:prior}, P(W$_i$), is the worker's average score over *m* accuracy assessment assignment:  
\begin{equation}
\mathrm{P(W_i) \propto \frac{1}{m}\sum_{j=1}^{m}score_j}
\end{equation}

Thus equation \ref{eq:prior} uses two measures of worker accuracy, 1) their overall average accuracy score multipled by 2) their average producer's accuracy to create a *weight* for their individual maps for the given site. Equation \ref{eq:main} becomes:

\begin{equation} 
\mathrm{P(\theta|D)=\frac{\sum_{i=1}^{n}weight_iP(\theta|D, W_i)}{\sum_{i=1}^{n}weight_i}}
\end{equation}

With $\mathrm{P(\theta|D, W_i)}$ being either 0 or 1. In labelling, if the consensus result for a pixel is: $\mathrm{P(\theta = field|D)}$ > 0.5, then we assign that pixel to the field category, otherwise to the no field category. 

After creating the consensus label, the degree of confidence in the resulting label value can be expressed in terms of Bayesian risk. Risk for a given training sample site *i* is calculated as:

\begin{equation}
\mathrm{R_i=\frac{1}{n}\sum_{j=1}^{n} C_j(1 - L_j) + (1 - C_j)L_j}
\end{equation}

Where *C* is the consensus probability that a given pixel *j* is a field ($\mathrm{P(\theta = field|D)}$), and *L* is the consensus label (i.e. non-field if *C* < 0.5, field if *C* > 0.5) for that pixel. *R* is thus the average risk across all pixels at a given site, and the slope of the risk varies depending on whether *L* is a field or not a field (Figure \ref{fig:riskcurve}). The closer to 0 the lower the risk that the *L* is mislabelled, while values approaching 1 indicate increasing risk of mislabelling.  

```{r riskcurve, echo = FALSE, fig.align='center', out.width="70%", message=FALSE, warning=FALSE, fig.cap="Bayesian risk values (Y-axis) for consensus values (X axis) ranging from 0 to 1 (0 indicates no consensus that a pixel falls into the field class, 1 means complete consensus) for field and non-field consensus labels."}
knitr::include_graphics('figures/si_label_risk.png')
```

#### Example

To provide an example of this approach in practice, we'll imagine two workers A and B, each with the following histories:

```{r, echo = FALSE}
library(magrittr)
sc <- tibble::tibble(Worker = LETTERS[1:2], `Prod. Acc. (Field)` = c(0.8, 0.62),
                     `Prod. Acc (no field)` = c(0.81, 0.61), 
                     Score = c(0.75, 0.60))
wtA <- sc[1, ]$Score * sc[1, ]$`Prod. Acc. (Field)`
wtB <- sc[2, ]$Score * sc[2, ]$`Prod. Acc (no field)`

# sc
knitr::kable(sc, align = c("r", "c", "c"), format = "pandoc")
```

In this scenario, worker A thinks that the given pixel falls within a field, and worker B thinks it is not a field.  First, we calculate the weights for each worker:

$$
Weight_A = score_A * PA_A(field) = P(W_A)P(D=field|W_A) = `r sc[1, 2]` * `r sc[1, 4]` = `r wtA`
$$

$$
Weight_B = score_B * PA_B(field) = P(W_B)P(D=no field|W_B) = `r sc[2, 3]` * `r sc[2, 4]` = `r wtB`
$$

And then we plug these weights into the full equation:


$$
\mathrm{P(\theta|D)=\frac{\sum_{i=1}^{n}weight_iP(L = field|D, W_i)}{\sum_{i=1}^{n}weight_i}} = \frac{`r wtA` * 1 + `r wtB` * 0}{`r wtA` + `r wtB`} = `r round((wtA * 1 + wtB * 0) / (wtA + wtB), 3)`
$$

Since `r rj <- round((wtA * 1 + wtB * 0) / (wtA + wtB), 3); rj` > 0.5, we label the particular pixel a field.  



Using the relevant part of equation 15, T=the corresponding risk associated with this particular pixel's label $\mathrm{R_j}$ is thus `r rj * (1 - 1) + (1 - rj) * 1`. 





### Accuracy assessment
We designed and implemented a map accuracy assessment protocol following procedures summarized by @StehmanKeyissuesrigorous2019. This entailed the creation of a map reference sample, which first entailed designing a sample, and then designing how the sample response would be collected.

#### Map reference sample design
A reference sample should ideally have some randomization involved in the sample selection protocol, and the probabilities for a sample being included should be provided.  There are multiple sample designs that can be chosen, but we employed a stratified sampling design, using the segmented field boundaries to define the strata for field/non-field. To create the sample, we first extracted the centroids for each of the sample grid cells in Ghana. We then intersected the centroid points with the field segments, assigned a class of 1 (cropland) where points intersected a field, and 0 where they didn't (non-cropland). We then removed from this set of points all those that corresponded to model training, validation, or training reference sites. We then extracted a random sample from both the cropland and non-cropland points. To determine the sample size of each, we specified a desired confidence interval using the following formula [@StehmanKeyissuesrigorous2019]:

$$n = \frac{z^2p(1-p)}{d^2}$$
        
Where p is the estimated probability (or mapped class accuracy), and d is the size of the margin of error (1/2 the confidence interval). We selected a d value of 0.03 and assumed that the user's accuracy of the field class would be 0.75 and that of the non-cropland class would be 0.8, returning sample sizes of 800 and 683, respectively. The distribution of the resulting map reference sample is shown in Figure \ref{fig:refsample}.

```{r refsample, echo = FALSE, fig.align='center', out.width="80%", message=FALSE, warning=FALSE, fig.cap="Distribution of the selected map reference sample for the cropland and non-cropland class. "}
knitr::include_graphics('figures/si_map_reference_sample.png')
```

#### Response design
##### Stage 1
We collected the map reference sample on a separate instance of `labeller` set up for the purpose (`validator.crowdmapper.org`). For the sampling unit, we selected a rectangular polygon of ~0.1 ha (0.0002866$^\circ$ resolution), centered on the centroid of each grid cell selected for map reference sample.  Four classes were established for the validation: **cropland**, **non-cropland**, **uncertain but likely cropland**, and **uncertain but likely non-cropland**. The latter two classes were designed to capture information related to swidden dynamics, following the rationale that uncertainty and time since last cropping are likely to be positively correlated. This uncertainty also captures information about the inherent difficulty of the mapping task. Samples were collected by visually interpreting the overlays of PlanetScope composites presented in `labeller`, following the same interpretation protocols used by the labelling team, with the exception that the polygons placed were square and of 0.0002866$^\circ$ resolution. 

The map reference sample was first placed and defined in an initial stage. In this stage, the class of the initial square polygon was determined, and then the polygon was adjusted if it overlapped more than one class. In sites where the initial map reference grid fell on one of the two uncertain classes, a second polygon was placed in the nearest location where a definite cropland or non-cropland interpretation could be made, with the choice determined by whether the uncertain class was more likely cropland or not cropland. The uncertain class was only used for providing insight into fallow cycles, while the second, more certain polygon was used in a second stage of the map reference sample, which was given to multiple independent interpreters (see below). 

The following set of rules were followed in this first, sample placing phase (undertaken by Lyndon Estes):

1. When determining the class corresponding with the initial location of the target grid, IF:
  - More than half of the target falls within what appears to be a clear arable crop field, then assign it to **cropland**. 
  - More than half falls in what is clearly not a field, then assign it to **non-cropland**. 
  - More than half falls in a location where it is harder to tell whether it is cropland or non-cropland, determine whether it is more likely a crop field or not a crop field, and then assign either **uncertain but likely cropland**, or **uncertain but likely non-cropland**.
  
2. After determining the class, IF:
  a. The target polygon is contained entirely within a single clear class AND
      - The class is either **cropland** or **non-cropland** THEN:
        - Digitize a point within the center of the target box, assign appropriate class, and complete the assignment
      - The class belongs to one of the two **uncertain** classes THEN:
      
        - Digitize a square aligned exactly with the initial target polygon, and choose the class **cropland** or **non-cropland** (following the most likely interpretation) and move the new polygon to the nearest location where it can be contained entirely within that clear **cropland** or **non-cropland** class;
        
        - Digitize a point in the middle of the target polygon, and assign it the appropriate label: **cropland** or **non-cropland**. 
        
  b. The target polygon partially overlaps a second class AND:
      - That class is either **cropland** or **non-cropland** THEN:
      
        - Digitize a square aligned exactly with the target polygon, assign the appropriate class, and move the box to the nearest location (shortest possible distance) where it can be contained entirely within that same class.  
  
      - That class is **uncertain** THEN:
    
        - Digitize a square aligned exactly with the target polygon, assign the appropriate **uncertain** class, then move the box to the nearest location (shortest possible distance) where it can be contained entirely within that same **uncertain** class.
        - Digitize a second square aligned exactly with the initial target polygon, and choose the class **cropland** or **non-cropland** (following the most likely interpretation) and move the new polygon to the nearest location where it can be contained entirely within that clear **cropland** or **non-cropland** class.
  

We then 
  - Definition of spatial assessment
  - Definition of classes
  - Reference source
  - Specific information collected from each source
  - Rules for reference class labels
  - How agreement between map and reference classes is defined

##### Stage 2



\clearpage
# Results
## Image quality

## Active learning process
### Labelling
The distributions of the initial 500 sites used to train the RandomForest model for each AOI are shown in Figure \ref{fig:trainval}. In AOI 3, the initial active learning cycle resulted in low accuracy because the northern part of the AOI shows low contrast between fields and the surrounding vegetation in the dry season. Training the model with the initial 500 samples resulted in large commission errors in this part of the AOI, thus we ran a second active learning cycle that began with an inital random draw of 300 training sites confined to this AOI (blue points in Figure \ref{fig:trainval}A)
```{r trainval, echo = FALSE, out.width="80%", fig.cap="The distribution of A) initial randomly selected training sites, including 300 points selected within AOI 3 to retrain a second active learning run for this region, as well as the locations of training reference sites, and B) validation points and training sites selected during each active learning iteration.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_training_validation_pts.png')
```

The distribution of data collection effort and scores received at training reference sites by labelling team (Figure \ref{fig:trainqual}).

```{r trainqual, echo = FALSE, out.width="60%", fig.cap="The A) distributions of scores at training reference sites for each labeller (means indicated by X in boxplots), and B) the number of training and training reference sites completed by each labeller. Labellers' identities are anonymized.", fig.align='center', message=FALSE, results='hold'}
knitr::include_graphics('figures/si_training_quality_n.png')
```


## Experiments
### Label quality

```{r, echo = FALSE, out.width="100%", fig.cap="Cropland probability images produced by RandomForest models trained with A) consensus labels, B) the most accurate worker's labels, and C) the least accurate worker's labels.", fig.align='center', message=FALSE}
knitr::include_graphics('figures/si_hml_probs.png')
```


\clearpage
# References
\singlespace


<div id = "refs"></div>


\eleft

\clearpage





```{r sessioninfo, echo = FALSE, eval = FALSE}
# set eval = FALSE if you don't want this info (useful for reproducibility) to appear 
sessionInfo()
```
