---
title: "Improving cropland maps through tight integration of human and machine intelligence"


author:  

- name: \*Provisional author list\*
  affilnum: '1'

- name: Lyndon D. Estes
  affilnum: '1'
  email: lestes@clarku.edu

- name: Su Ye
  affilnum: '1'

- name: Lei Song
  affilnum: '1'

- name: Ron Eastman
  affilnum: '1'
  
- name: Sitian Xiong
  affilnum: '1'

- name: Tammy Woodard
  affilnum: '1'

- name: Boka Luo
  affilnum: '1'

- name: Dennis McRitchie
  affilnum: '2'

- name: Ryan Avery
  affilnum: '3'

- name: Kelly Caylor
  affilnum: '3'

- name: Stephanie Debats
  affilnum: '4'

- name: SpatialCollective
  affilnum: '5'

- name: Meridia
  affilnum: '6'

- name: Azavea
  affilnum: '7'

affiliation:

- affilnum: 1
  affil: Graduate School of Geography, Clark University, 950 Main Street, Worcester, MA 01610 USA

- affilnum: 2
  affil: Dennis and Sons, Tucson, AZ, USA

- affilnum: 3
  affil: UCSB

- affilnum: 4
  affil: Uber

- affilnum: 5
  affil: SpatialCollective

- affilnum: 5
  affil: Meridia

- affilnum: 5
  affil: Azavea

output:

  pdf_document:
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_tex: yes
    number_sections: yes
    template: manuscript.latex
    includes:
      in_header: header.tex

  html_document: null
  
  word_document: null

documentclass: article
classoption: a4paper
capsize: normalsize
fontsize: 11pt
geometry: margin=1in
linenumbers: yes
spacing: doublespacing
footerdate: yes
abstract: The abstract can go either here or below
keywords: rmarkdown, reproducible science
bibliography: 
  - references.bib
  - knitcitations.bib
csl: ecology.csl

---


```{r setup, include=FALSE, cache=FALSE, message = FALSE}

library(knitr)
library(citr)

#opts_knit$set(root.dir=normalizePath('../'))

### Chunk options: see http://yihui.name/knitr/options/ ###

## Text results
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)

## Code decoration
opts_chunk$set(tidy = TRUE, comment = NA, highlight = TRUE)

## Cache
opts_chunk$set(cache = 2, cache.path = "output/cache/")

## Plots
opts_chunk$set(fig.path = "output/figures/")


```


```{r knitcitations, echo=FALSE, cache = FALSE}
library(knitcitations)
cleanbib()   
cite_options(citation_format = "pandoc")
```


<!-- \singlespace -->

<!-- \vspace{2mm}\hrule -->
<!-- \vspace{-20pt} -->
<!-- <!-- # Abstract --> 


<!-- \vspace{3mm}\hrule -->

<!-- *Keywords*: rmarkdown, reproducible science -->

\doublespace

\bleft

# Introduction
How much cropland is on the planet, and where is it located? How much more will be needed to meet humanity's rapidly growing food demands? The answer to the first question is unclear, as existing estimates tend to vary widely and cropland maps show substantial spatial disagreements [e.g. @FritzHighlightingcontinueduncertainty2011; @Fritzneedimprovedmaps2013], which makes it difficult to answer the second. The first question can't be clearly answered because cropland estimates rely heavily on remote sensing-derived landcover maps, which can be notoriously high in error, particularly over regions such as Africa [@Esteslargeareaspatiallycontinuous2018; @FritzComparisonglobalregional2010], where agricultural changes will be largest and the need for an accurate baseline is thus greatest [@EstesReconcilingagriculturecarbon2016; @SearchingerHighcarbonbiodiversity2015]. *[Fill in more reason why cropland maps are important]*  

<!-- - The problem in general  -->
<!--     - Africa in particular -->
<!--     - Types of analyses that depend on cropland maps -->

Cropland mapping over a region such as Africa is difficult for several reasons. The primary reason relates to the characteristics of the region's smallholder-dominated croplands, where field size averages 1-2 ha [@Debatsgeneralizedcomputervision2016; @FritzMappingglobalcropland2015]. This size is small relative to the 30-250 m resolution of the sensors typically used for most landcover mapping efforts, which results in errors due to mixed pixels and aspects of the modifiable area unit problem [@Openshawmillioncorrelationcoefficients1979], in this case the pixel's shape may be poorly matched to that of cropland, and is too coarse to aggregate to approximate that shape at the characteristic scales of crop fields [@Darkmodifiablearealunit2007; @Esteslargeareaspatiallycontinuous2018]. On top of the matter of scale is 1) high intra-class variability of the cropland class, compounded by the fact that these particular croplands can be heavily intergraded with surrounding vegetation [@Debatsgeneralizedcomputervision2016; @Estesplatformcrowdsourcingcreation2016], and 2) the substantial temporal variability within croplands, both within and between seasons. These latter two aspects pose challenges for the classification algorithms that are applied to the imagery.  

These problems arising from cropland characteristics are increasingly being overcome due to technological advances. Recent advances in satellite technology have increased the coverage of high (<5 m) or near-high (10 m) resolution imagery with weekly to near-daily return intervals [@DruschSentinel2ESAOptical2012; @McCabefutureEarthobservation2017]. This spatial and temporal resolution addresses the sensor-scale mismatch, and more effectively captures the intra-seasonal dynamics of cropland, which helps classifiers distinguish cropland from surrounding cover types [@Debatsgeneralizedcomputervision2016; @Defournyrealtimeagriculturemonitoring2019]. On top of this, advances in cloud computing [@GorelickGoogleEarthEngine2017], the opening of image archives [@WulderglobalLandsatarchive2016], and next generation machine learning approaches [@MaxwellImplementationmachinelearningclassification2018; @ZhuDeeplearningremote2017] are placing large volumes of these moderate to near-high resolution imagery together with the computational and algorithmic resources necessary to classify them at scale. These capabilities are aleady being used to create a new generation of higher resolution (20-30 m) cropland and landcover maps for Africa and other regions [@ChenGloballandcover2015; @ESAESACCILAND; @LesivEvaluationESACCI2017; @XiongNominal30mCropland2017].  

Despite these advances, the highest resolution (<5 m) image sources are still not used to map cropland over very large extents, presumably because they are commercial and relatively high cost to acquire, in addition to the greater computational demands. As such, map accuracy can still be a challenge, particular for user's accuracy, which ranges between 46 and 76% for for the cropland class [e.g. @LesivEvaluationESACCI2017; @XiongNominal30mCropland2017]. 

Accuracy may also suffer due to error-inducing factors that are becoming somewhat more pronounced as a consequence of these technology advances, particularly with respect to algorithms. Advances in machine learning are helping to greatly improve classification skill, but these algorithms generally require large training datasets [@MaxwellImplementationmachinelearningclassification2018], particularly neural network-based "deep-learning" methods (cite). To satisfy this need for more training (and reference) samples, map-makers increasingly rely on visual interpretation of high resolution satellite or aerial imagery to collect training (or validation) samples [e.g. @XiongNominal30mCropland2017; @ChenGloballandcover2015; @StehmanKeyissuesrigorous2019]. Several web-based platforms have been developed to facilitate such efforts, which provide convenient and highly scalable tools for training data collection [e.g. @BeyCollectEarthLand2016; @Estesplatformcrowdsourcingcreation2016; @FritzGeoWikionlineplatform2012]. Visually interpreted training labels present two particular problems. The first is that such labels have inevitable interpretation errors that can vary substantially according to the skill of the interpreter [@Estesplatformcrowdsourcingcreation2016; @WaldnerConflationexpertcrowd2019]. These errors are typically not accounted for in reported accuracy metrics (cite Arthur's paper), thus the degree to which such labelling error impacts map accuracy is unclear. However, a study of how land cover map error propagates through subsequent analyses [@Esteslargeareaspatiallycontinuous2018] suggests training data quality could have a large impact. The second problem is that visual interpretation depends on high resolution imagery (<5 m), as lower resolutions make it difficult for a human analyst to discern cropland. Typically the only practical source for such imagery are "virtual globe" basemaps provided by Microsoft and Google, which are composed of mosaics of various high resolution satellite and aerial image sources that typically span 3-5 years of time within a single country [@LesivCharacterizingspatialtemporal2018]. This within-mosaic temporal variation can set up a temporal mismatch between the imagery being interpreted and the imagery being classified, which are usually from a different source (e.g. Landsat, Sentinel; @XiongNominal30mCropland2017) and often represent a different year. If a land change occurs in the interval between the two image sets (e.g. a new field was created), this can introduce error into the training data that is then passed on to the classifier. This source of error may be elevated in smallholder-dominated systems, where swidden practices are common (cite), or in rapidly developing agricultural frontiers [@ZengHighlandcroplandexpansion2018].  
<!-- - Why cropland mapping is hard -->
<!--     - 3 problems -->
<!--         - The target itself and its spectral variability and the spectral variability of its background -->
<!--             - Spectral and spatial resolution of sensors -->
<!--             - Poses a challenge for classification algorithms, which want both high resolution to be smaller than characteristic field size, but temporal contrast also -->


To improve the accuracy of cropland maps over smallholder-dominated systems therefore requires an approach that meets three requirements. First, it should be based on high spatial and temporal resolution imagery, to be able to capture the fine grain and temporal variability of these fields. Second, an algorithm with suitable skill for classifying these images must be selected, and combined with the computational resources needed to process large imagery volumes. Third, a method for collecting large volumes of high quality training and validation data based on image interpretation is essential. This method should quantify and minimize the errors associated with image interpretation. It should also ensure that labels are collected either from the same imagery that is being classified, or from contemporanous imagery, in order to reduce errors introduced by land change processes. 

<!-- - Training data dependence issues -->
<!--     - Also interact with image resolution issues -->
<!--     - Increasingly rely on high amounts of training data -->
<!--     - Human interpretation error (training data quality) -->
<!--       - Temporal mismatch with classification feedstock (Lesiv paper) -->
<!--       - Errors in interpreting these sources of imagery -->
<!-- - Review of who does what to solve the problem (probably integrated with problem statement) -->
<!--   - croplands.org -->


We describe here a cropland mapping system that addresses these requirements on the way to creating an object-based map of individual fields The first requirement is enabled by the recent availability of cubesat data that provides 3-4 m resolution imagery over large areas at near daily intervals [@McCabefutureEarthobservation2017]. Although these data are of lower spectral depth and quality than Landsat, Sentinel, or Worldview imagery, they enable country-continent scale image mosaics to be created for multiple periods during the crop growing calendars, and this representation of intra-annual variability is more important for cropland classification than spectral depth [@Debatsgeneralizedcomputervision2016]. Additionally, despite this imagery being up to ~16 times coarser than much of Bing or Google imagery, it is sufficiently resolved for humans to discern most fields [e.g. see @Esteslargeareaspatiallycontinuous2018; @FourieBetterCropEstimates2009]. This allows labels to be generated on the same imagery processed by the classifier, thereby addressing one of the two needs related to training data (requirement 3). The second requirement is addressed by a computer vision/machine learning classifer that is effective for classifying smallholder croplands [@Debatsgeneralizedcomputervision2016] and was re-engineered to run on high performance, cloud-based computing clusters. This classifier is tightly coupled to a front-end platform for collecting label data, which includes rigorous accuracy assessment protocols [@Estesplatformcrowdsourcingcreation2016] and a novel approach for merging multiple maps into consensus labels, thereby minimizing image interpretation error. The training and machine learning components are combined within an "active learning" framework, wherein the machine learning process assesses classification uncertainty in unlabelled areas after a training step, and selects sites from areas of highest uncertainty for additional labelling [@DebatsIntegratingactivelearning2017]. Our framework automates and formalizes this interactive approach to label selection, which has been shown to be important for improving accuracy in previous studies [e.g. @XiongNominal30mCropland2017; others add]. Furthermore, it is more efficient than purely randomized or conventionally stratified approaches to label collection, meaning that fewer labels are needed to generate a given level of accuracy [@DebatsIntegratingactivelearning2017]. Finally, an unsupervised  segmentation step is applied to the imagery and merged with the pixel-wise classifications from the machine learning process, **resulting in a further sharpening of accuracy** and resulting in a final vector-based field boundary map.

We demonstrate this approach to map cropland in Ghana, a country where agriculture is predominantly small-scale and comprises a broad mix of agricultural systems, including large areas where shifting agricultural practices predominate [@KansangaTraditionalagriculturetransition2019; @SambergSubnationaldistributionaverage2016]. These factors make Ghana emblematic of the substantial challenges facing cropland classification over the broader region. 

<!-- - Our approach that tries to solve these three problems -->
<!-- - Within a framework that tightly integrates tightly training data collection -->
<!-- - And handles large-scale computation -->
<!-- - Segmented boundaries -->
    
# Methods
## System overview 
The mapping system is centered around four major components: 1) an image acquisition and pre-processing component, 2) a training data collection platform, 3) a machine learning component, and a 4) segmentation component. The first component is applied as a once-off step to generate an image catalog necessary for the mapping geography, while the second component initiates the the interactive image training and classification (component 3) pipeline that results in a pixel-wise posterior probability map of cropland probabilities on its final iteration (Fig. 1). The final segmentation step (component 4) is then initiated and applied to both the pre-processed primary images and the posterior probability maps, resulting in the final vectorized fields boundary maps. Each system component comprises an individual module software hosted on a GitHub repository (see data and software availability section for details). 

## System components
### Imagery
**[Note: This will require updating to reflect changes to compositing methods]**
The primary image source used in our system is PlanetScope Analytic surface reflectance product [@PlanetTeamPlanetApplicationProgram2017], although can be readily adapted to apply to Sentinel and Landsat images. PlanetScope provides three visual (red, green, blue) and a near-infrared band at 3-4 m resolution at nominal daily frequency. Although these images are substantially pre-preprocessed and corrected for atmospheric effects, there are substantial residual errors from inter-sensor differences and the radiometric normalization process [@HouborgDailyRetrievalNDVI2018], substantial variation in the orientation of scene footprints, as well as a high frequency of cloud cover over the study region [@WilsonRemotelySensedHighResolution2016]. To correct for these factors, we developed a procedure for creating temporal composites representing the primary growing and non-growing seasons within a single year.  

```{r, echo = FALSE, out.width="50%", fig.cap="An overview of the primary system components, the data stores that hold the inputs and outputs from each component, and the direction of connections between them. The dashed line indicates iterative interactions, while solid lines indicate one-time or irregular connections.", fig.align='center'}
knitr::include_graphics('figures/figure1.png')
```

PlanetScope imagery is accessed via the Planet API, and an initial order is placed wherein the imagery representing all available dates in the two compositing periods and within a bounding box covering the mapping geography is collected and transferred directly to the system's cloud storage (hosted on Amazon Web Services S3). 

We then transform all images into an "analysis ready data" (ARD) product [@DwyerAnalysisReadyData2018] that is prepared against a tiling grid that is situated within a larger 1 degree resolution grid that covers the entire continent. Each 1 degree cell defines the minimum mapping area of interest (Fig. 2A) is divided into 400 0.05 degree cells (Fig. 2B), which provides the scale for creating image composite tiles. The ARD process simply clips all available imagery intersecting each 0.05 cell to the extent of that cell, regardless of cloud cover. 

<!-- and then applies the usable data mask (UDM) supplied with the imagery to remove low quality and cloudy pixels.  -->

```{r, echo = FALSE, out.width="100%", fig.cap="An overview of analytical scales used in the mapping platform, including the 1 degree areas of interest defining the minimum mapping geography (A), the 0.05 degree grid used for creating image composite tiles (B), and the 0.005 degree resolution reference grid used for collecting training data and distributed computing (C).", fig.align='center'}
knitr::include_graphics('figures/figure2.png')
```
The temporal compositing process is applied to the ARD imagery for two time periods, the dry season period (December-February) and the entire wet season (May - September). The longer period is necessary for the wet season because of the high frequency of cloud cover, which severely limits the number of clear scenes over any given area. For each pixel in each image in each ARD temporal stack, we calculate two weights, with the first being:

<!-- values. The first is based on a simplified formula of the Haze Optimized Transformation (HOT) index [@Zhangimagetransformcharacterize2002; @ZhuObjectbasedcloudcloud2012]:  -->

\begin{equation} \label{eq:cloud}
\mathrm{W1_t} = \frac{1}{\mathrm{blue_t}^2}
\end{equation}

And the second:

<!-- Equation solution from here: https://stackoverflow.com/questions/4027363/two-statements-next-to-curly-brace-in-an-equation -->
 
\begin{equation} \label{eq:shadow}  
\mathrm{W2_t} =\begin{cases}
    1, & \text{if $\mathrm{NIR_t}$ < median\{$\mathrm{NIR_1}$, $\mathrm{NIR_2}$, ..., $\mathrm{NIR_i}$\}}.\\
    \frac{1}{\mathrm{NIR_t}^4}, & \text{otherwise}.
  \end{cases}
\end{equation}

Where *t* is a particular date in near-daily time series of PlanetScope images, which begins at date 1 for the given compositing period and ends on date *i*, *blue* is the blue band, and *NIR* the near infrared band. Equation \ref{eq:cloud} assigns lower weights to hazy and clouded pixels, while equation \ref{eq:shadow} assigns low weights to pixels in cloud shadow. 

Once these two weights are calculated, the final composited pixel value for each of the four PlanetScope bands is calculated as:

\begin{equation}
\mathrm{\bar{B} = \frac{\sum_{t=1}^{T}B_t * W1_t * W2_t}{\sum_{t=1}^{T}W1_t * W2_t}}
\end{equation}

Which is a weighted mean for each pixel for each band *B* for the particular compositing period. The composited tiles are then added to the S3 store (Fig. 1), where they are stored as cloud-optimized geotiffs, and a "slippy map" rendering is created for each composite using RasterFoundry (cite). This web-based rendering of the image is used in the training data platform (see section X). 


### Labelling platform
Both training and reference data are collected by a custom crowdsourcing platform, which we refer to here for simplicity's sake as *labeller*. The original version of *labeller* was designed to work with Amazon Web Services' Mechanical Turk job marketplace [@Estesplatformcrowdsourcingcreation2016]. The basic structure of the system remains the same, but we have re-engineered it to be a standalone platform that 1) allows us to register and pay workers (the human supervisors) directly to conduct digitizing work, and 2) drives and assesses the overall machine learning process. The system runs on a Linux Virtual machine hosted on an AWS EC2 instance and is comprised of a database (PostGIS/Postgres), a mapping interface (OpenLayers 3), an image server (RasterFoundry), and a set of utilities for managing mapping and assessing assignments and converting them to rasterized labels for the machine learning process. Each *labeller* instance is focused on a specific mapping geography (e.g. one to several 1 degree cells in Fig. 2A), thus several instances are set up to run in parallel to cover a larger mapping geography such as Ghana. 

Here we provide an overview of *labeller*'s architecture within the context of the labeling workflow within a single instance.

#### Mapping workflow
##### Selecting training and reference sites
The labelling process begins with a python script that randomly selects a subset (e.g. 100) of the 0.005 degree grid (Fig. 2C) cells that intersect the *labeller* instance's specific area of interest (e.g. shaded grey cell in Fig. 2A). These site names are stored in a Postgres table that represents the mapping queue. Another python process converts each site into a mapping *task* that has a specified number of *assignments* (maps drawn by an individual worker) that must be completed before the task is complete.  


##### Mapping assignments
Workers registered in the system log in to the mapping platform (built with Flask) and navigate to the OpenLayers-based field mapping interface (Fig. 3). There the worker is presented with a white target box representing the 0.005 degree cell that was randomly selected to label, a set of digitizing tools, and different image backdrops, which include false and true color renderings of both the growing season and off-growing season PlanetScope images covering that cell, as well as a basemap provided by Bing maps. The worker uses the polygon drawing tool to digitize the boundaries in crop fields visible within the PlanetScope overlays that intersect the grid target, following a set of pre-defined digitizing rules. To aid with interpretation, the worker toggles between the various seasonal renderings and the satellite base map to help form a judgement about what constitutes a field within the PlanetScope imagery. The worker assigns each completed polygon a label that describes the field class (e.g. annual cropland, tree crop, fallow), and after completing all visible fields saves the full set of polygons to the server. The worker is then automatically  taken to their next assignment at a different randomly selected site. 


```{r, echo = FALSE, out.width="100%", fig.cap="An overview of mapper's interface", fig.align='center'}
knitr::include_graphics('figures/figure3.png')
```

##### Processing completed assignments
Upon submission, the workers' polygons, if there are any--the worker will submit the assignment without mapping if they see no fields in the image--are cleaned to fix any topological irregularities that arose during digitization (see supporting information (SI)) and stored in a PostGIS table. The assignment that was just completed represents one of two possible types of tasks: 1) an accuracy task; 2) a labeling task for training machine learning. If is was the former type, an `R` script is called that executes a series of map comparisons between the worker's polygons and the reference polygons, assigning a score as based on the following formula:

\begin{equation}
\mathrm{score_i}=\beta0\mathrm{I}+\beta1\mathrm{O}+\beta2\mathrm{F}+\beta3\mathrm{E}+\beta4\mathrm{C}
\end{equation}

Where *i* represents the particular assignment, and $\beta_{0-5}$ represent varying weights that sum to 1. *I* refers to "inside the box" accuracy, *O* is the accuracy of those portions of the worker's polygons extending beyond the target grid boundaries, *F* is fragmentation accuracy, a measure of how many individual polygons the worker delineated relative to the reference, *E* measures how closely each polygon's boundary matched its corresponding reference polygon boundary, and *C* assesses the accuracy of the worker's thematic labels (see SI for individual formulae).

Over time, workers are assessed multiple times across a range of accuracy tasks that are selected to represent the variability of the agricultural system being mapped. Each worker's score history is averaged to provide an overall assessment of accuracy, and this information is used for creating labels, the second task. 

If the worker's completed assignment was a labelling task, their maps remain stored in the database until the remaining assignments needed to complete the task are submitted by other workers. Once complete, an R-based consensus labelling routine is invoked, which combines the maps from all workers who undertook the task into a single consensus label using a Bayesian merging approach:

\begin{equation}
P(\theta|\mathrm{D})=\sum_{i=1}^{n}\mathrm{P}(\mathrm{W_i}|\mathrm{D})\mathrm{P}(\mathrm{\theta}|\mathrm{D}, \mathrm{W_i})
\end{equation}

Where $\theta$ represents the true cover type of a pixel (field or not field), *D* is the worker's label of that field, and $W_i$ is an individual worker.  P($\theta$|D) is therefore the probability that the actual cover type is what the workers who labelled it says it is, while P(W$_i$|D) is the worker's average score over the accuracy assessment assignments they have mapped to date, and P(W$\theta$|D, *W$_i$*) is the worker's label for that pixel. This approach therefore uses the overall accuracy of each worker to weight their labels as they are combined with other workers' labels for a given pixel (see SI for further details of equation). 

Once a the full set of sites has been completed, they are randomly partitioned into training and validation sets according to predetermined proportions. 

<!-- - Training and validation data -->
<!-- - Accuracy assessment -->

### Classification pipeline
Upon completion of the labels, *labeller* automatically launches an ephemeral Elastic Map Reduce cluster consisting of tens to hundreds of instances, depending on the size of the mapping geography. We identifed an optimal cluster of 50-60 instances (m4.2xlarge) each having 8 virtual CPUs and 30 GB RAM, which ensured minimal idle time for any single node. 

#### Feature extraction
The first step is the extraction of additional features from the growing season/non-growing season PlanetScope imagery. Previous work showed that a large number of simple features that summarize the statistical properties of reflectance and vegetatiation indices in local neighborhoods are highly effective for classifying smallholder croplands [@Debatsgeneralizedcomputervision2016]. We followed this logic in this study, but opted for a smaller feature space to accomodate the storage and memory requirements of a mapping geography orders of magnitude larger than that initial work. Here, we used a set of 24 features, comprising the eight original composite bands (four from each season), an 11X11 mean of each band, as well as the standard deviation of each band within a 5X5 window (Table X).


\begin{center}Table X. List of features.\end{center}

Feature             Window Size  N Features
------------------- ------------ ------------
RGB-NIR                      1X1            8 
Mean                       11X11            8
Standard deviation           5X5            8
------------------ ------------- ------------

Feature extraction and the conversion of image features is handled by a combination of `GeoTrellis`, `GeoPySpark`, `rasterio`, and `RasterFrames`. These collectively extract subsets of imagery from the PlanetScope temporal composites, derive the features (Table X), and convert these into `RasterFrames`, which are an Apache Spark compatible data object that is designed specifically for raster data. Features are extracted on the fly for each cell in the training and validation sets, a functionality enabled by storing the image composites as Cloud-optimized Geotiffs (COGs), which allows fast windowed reads of image subsets.  

#### Classification
Once the features from the training sites are extracted into `RasterFrames`, these are combined with their corresponding labels and passed to the machine learning classifier, a `SparkMLlib` implementation of RandomForest [@BreimanRandomForests2001]. \textcolor{red}{The model is trained with a balanced sample and a tree depth of 15 and total tree number of 80, which initial testing showed to provide a reasonable balance between computational time and performance.}

After fitting, the model is applied to the features of the model validation set, and a set of accuracy metrics are calculated, including ordinary binary accuracy and the true skill statistic [@AlloucheAssessingaccuracyspecies2006], which is considered to be more effective than the Kappa statistic in that it controls for class prevalence. 

### The active learning loop
After fitting and model evaluation a second prediction process is undertaken that initiates the active learning process. The feature extraction process is repeated for the rest of the mapping geography that falls outside of the training and validation sets, but applied to a subset of randomly drawn pixels from each cell in order to reduce computation demand. The fitted model is then applied to these features in order to calculate the posterior probability of cropland presence, and an uncertainty criterion [@DebatsIntegratingactivelearning2017] is calculated for each grid cell:

\begin{equation}
\mathrm{Q_I = \sum_{I(x, y) \epsilon I} (p(x, y) - 0.5)^2}
\end{equation}

```{r, echo = FALSE, eval = FALSE}
uncertainty <- function(x) sum((x - 0.5)^2)
a <- runif(10, min = 0.4, max = 0.6)
b <- c(runif(5, min = 0, max = 0.2), runif(5, min = 0.8, max = 1))

# sum(a - 0.5)^2
# 1 - sum(a - 0.5)^2
# 1 - sum(b - 0.5)^2
# ass <- (a - 0.5)^2
# bss <- (b - 0.5)^2
# 1 - sum(ass)
# 1 - sum(bss)
# (1 - sum(b - 0.5)^2)
uncertainty(a)
uncertainty(b)
```
Where Q is the uncertainty for cell I, calculated from the predicted probability *p* of the randomly selected subset of pixels (x, y) drawn from it. Pixels with predicted probabilities closer to 0.5 are least certain as to their classification, thus images having the lowest values of $Q_I$ represent sites posing the most difficulty for the classifier. 

After scoring with the uncertainty criterion, the top *N* most uncertain grid cells are selected and sent back to *mapper*, where workers then digitize new labels. These new consensus labels are then added to *learner*'s initial training pool upon completion, and a new machine learning cluster is launched, resulting in a reassessment of uncertainty for unlabelled sites, a selection from the most uncertain sites, and a labelling of those sites by the workers. This loop repeats until the map accuracy metrics, as assessed against the validation set, show no further gains.  

### Segmentation
After the final iteration, the segmentation algorithm is invoked, which entails several steps. In the first step, the meanshift algorithm [@YizongChengMeanshiftmode1995a] is applied to the original bands of the growing season/off-growing season PlanetScope image pair. 

In the second step, a Sobel filter is then applied to the green, red, and near-infrared mean-shifted bands [**I assume**], and weighted edge image is computed using weights of 1, 1, and 2 to each band for each season [**for both growing season on and off?**]. A watershed algorithm (**cite**) is then run on the weighted edge image, with a high level of segmentation specified (**2400** segements per tile). 

In the third step, a region adjacency graph is constructed for each tile, in which each node represents all pixels within each polygon created in the previous step. The edge between two adjacent regions (polygons) is calculated as the difference between the means of normalized colors of all bands. Hierarchical merging is then applied, in which the most similar pairs of adjacent nodes are merged until there are no edges remaining below a predetermined threshold of **X**.  

In the fourth step, the merged polygons are overlaid with the posterior probabilities resulting from the final active learning loop, and polygons in which the average posterior probability is greater than a predetermined  threshold (typically 0.5, but is possible to vary locally) are retained as field polygons. 

In the final step, the retained polygons are refined by removing holes and smoothing their boundaries using the Douglas-Peucker algorithm (cite). Neighboring polygon that touch along tile boundaries are merged.

To assess the accuracy of the final segmented boundaries, we used a two-step approach. First, we assessed the overall thematic accuracy of the resulting classification against our map reference data. Second, to assess the quality of the segmentation, we compared the mean area and relative frequencies of the segmented polygons within different size classes against the same metrics derived from the digitized fields of the most accurate worker to create the given map. [**Note: the following maybe better for discussion**] We selected this relatively simple procedure, as opposed to more complex measures of object accuracy (**cite**), because, on the one hand, both the automated segmentation algorithm and human interpreter are cueing in on the same features--abrupt, physically detectable breaks within the imagery. On the other hand, no matter how well the intepreted/segmented boundaries align with the boundaries of fields in the imagery, it is logistically difficult and potentially impossible to evaluate performance against the boundaries that farmers use to delineate their fields, as the conception of a "field" may vary between farmers (**cite**) and their field edges may be invisible or span multiple units that appear to a segmentation to have distinct boundaries (**see SI**). 

### Accuracy assessment
**[Put in somewhere above text about training reference samples]**
The accuracy assessment procedure described in section 2.2 is used strictly to assess performance gains in the active learning loop, according to suggested best practice guidelines [@ElmesAccountingtrainingdata2019]. To assess the accuracy of final map products, following these same guidelines we used an independent map reference sample. To develop this sample, we first made a random selection of grid cells within each mapping AOI, with the selected number scaled relative to the AOI. Next, a primary supervisor examined the image composites within each randomly selected cell and placed three rectangular polygons of ~0.1 ha area in areas that were fully contained within in one of three classes: cropland, non-cropland, potentially cropland or non-cropland. In cases where less than three classes were present within the cell, the supervisor placed polygons in proportion to the dominant class. For example, in the case where the entire cell was taken up by non-cropland, the supervisor placed all three polygons in non-cropland. If two-thirds of the cell was non-cropland, and one-third ambiguous, two polygons would be placed in non-cropland, and one in the ambiguous class. After placing these polygons, the supervisor removed their classes from the polygons and sent them to three separate experts, who labelled the polygons according to their interpretation of the underlying imagery. The four sets of labels were then combined and the overall agreement between them was calculated. This agreement provided the upper bound on knowable map accuracy.

## Application and evaluation of the system
[Description of map creation process]
To create a map of Ghana, we divided the country into 16 distinct mapping zones, or areas of interest (AOIs), which were developed by grouping together each 1X1 degree cell (Figure 2A) fully contained with the tiles (Figure 2B) belonging to any adjacent 1X1 cells that overlapped with neighboring countries. The resulting AOIs ranged from 12,160 to 23,535 km$^2$ in extent (average = 15,457 km$^2$), and each was used to run a separate active learning process and to create a final segmented map. 

Prior to initiating active learning in each AOI, we grouped the AOIs into three zones: a northern zone comprising the 6 northernmost AOIs, a central to southeastern zone comprising the 3 middle and 3 southeastern AOIs, and a southwestern zone made up of the AOIs falling in the forest zone (see Figure SX in SI). Within each zone group, we randomly selected 500 points for initial labelling by the mapping team, which were used to train the first iteration of each RandomForest model run for the zone group. At the outset of mapping in each AOI, the mapping team labelled 100 randomly selected points within each AOI that were set aside for model validation, and subsequent iterations targeted new training sites for collection in each AOI based on classification uncertainty. 

To more fully evaluate the capacities of different components of the system, we performed several experiments. In the first experiments, we evaluated the degree to which training label quality impacts final map accuracy, by comparing maps created by training the RandomForest algorithm with labels created by the least accurate worker, the most accurate worker, in addition to those trained with the consensus labels. This test aligns with Tier 1 standards for accounting for the impact of training data error on final map accuracy, as recommended by @ElmesAccountingtrainingdata2019. Next, we evaluated the degree to which training sample selection via active learning impacted map accuracy relative to random selection of additional training sites. 

<!-- Related to this, we evaluated two different versions of the active learning uncertainty criterion to evaluate the impact on performance gain.  -->

<!-- Finally, we evaluated production performance in terms of speed of labelling and accuracy of results by comparing the generation time (label creation, training, and then classification) and accuracy of maps trained by 1) our core team of 10-12 experienced workers, and 2) a team of 100 novice workers from three different countries.  -->
 
# Results
```{r, echo=FALSE, message=FALSE, warning=FALSE}
tiles <- sf::read_sf(system.file("extdata/ghana_tiles.geojson", 
                                 package = "activemapper"))
km2 <- round(as.numeric(units::set_units(sum(sf::st_area(tiles)), "km2")))
km2 <- format(km2, big.mark = ",", scientific = FALSE)
```
We developed maps for all of Ghana (`r km2` km$^2$). Ghana has several agro-ecologically distinct regions, ranging from the primarily grain and vegetable crop producing regions in the northern savannas to tree crop-dominated system in the forested southwest, where cocoa and oil palm are among the dominant crops. For these latter regions, we did not attempt to classify tree crops, and instead mapped clearings that potentially contain field crops or recently felled or newly replanted tree crops. Our rationale for this decision was that the resolution of PlanetScope imagery is insufficient to distinguish such crops from the surrounding forest, and clear field boundaries are typically not visible in cocoa crops. 

## Image catalog

We generated a complete image catalog for the `r nrow(tiles)` tiles intersecting Ghana, spanning `r km2` km$^2$. This entailed downloading all PlanetScope imagery intersecting the total extent of these tiles for the periods May-September, 2018 (the growing season) and December, 2018 to February, 2019 (the subsequent dry season). The compositing processes was applied to render the resulting seasonally contrasting image pairs for each tile (**Figure Xa**). 

We evaluated the overall quality of the resulting composites across the region using a random selection of 100 tiles, which we graded according to a four category quality that evaluated the degree of residual 1) cloud and 2) cloud shadow, 3) the number of visible scene boundary artifacts, and 4) the proportion of the image in which resolution was degraded below the 3-4 m PlanetScope resolution, due to between-date image mis-registrations. Each category was qualitatively ranked from 0-3, with 0 being the lowest quality, and 3 the highest (see SI for complete protocol), thus the highest possible score was 12. The results showed that the majority of the sampled composites (**X%; Fig Xb**) received a score of **X** or higher (**Y and Z%**), as rated by four independent observers (mean standard deviation of observer scores was **XX**). Image quality varied spatially and temporally, with the lower quality images found during in the growing season period and the Southwest corner, where the substantially greater cloud cover resulting in a much lower density of available PlanetScope imagery for each time period (**Fig Xc**). Overall, the available image density and quality of composites were closely related (see SI for full details).  

```{r, echo = FALSE, out.width="70%", fig.cap="The location and quality scores of 100 randomly selected tiles for the growing (A) and off-growing season (B), and the corresponding distributions of the quality scores for each season, respectively (C and D) [This a hypothetical placeholder figure].", fig.align='center'}
knitr::include_graphics('figures/figure4.png')
```

## The active learning process
**[Fill in with intro]**. The average number of iterations per active learning process was **4**, as overall performance gains, as assessed against validation sets, saturated quickly (see **Figure X** in SI). 

The average computational time per iteration per AOI was *6 hours*. **Training time. Interaction time**

## Map accuracy
To assess the overall accuracy of the final maps, we used our reference set to evaluate the accuracy of classifications derived from thresholding the posterior probabilities produced by the RandomForest model, as well as the segmented field boundary maps. First, we evaluated the inter-rater agreement between reference supervisors in labeling reference polygons. The average inter-rater agreement was **X%, with a standard deviation of Y% (see Figure X in SI)**, which provides the upper bound on achievable map accuracy. 

Based on this reference sample, we found that the average accuracy of the pixel-wise classifications was **X, with a range of X-X** (Figure 5). Overall user's accuracy was slightly **lower/higher** at **X, (range X-X)**, while producer's accuracy averaged **X, (range X-X)**. Each accuracy measure was highest in **northern-northeastern 2/3** of the country, and lowest in **southwestern 1/3**, where rainfall and cloud-cover are highest, and where tree crops dominate. The segmentation process **increased/decreased** accuracy slightly, with a mean overall, user's and producer's accuracy of **X (range X-X), Y (range X-X), and Z (range X-X) percent**.

<!-- and an accuracy surface interpolated from the average cell-wise accuracies calculated between the predicted map and validation labels.  -->

```{r, echo = FALSE, out.width="80%", fig.cap="The overall, user's, and producer's accuracies (indicated by fill color) for each of the 16 mapping zones (zones numbers are shown on maps) [This a hypothetical placeholder figure].", fig.align='center'}
knitr::include_graphics('figures/figure5.png')
```

The accuracy patterns generally align with the difficulty of the labelling task, that is, **the southwestern regions were hardest for workers to interpret, as shown by Figure 5.**

## System evaluation
### Label quality and map accuracy
```{r, echo=FALSE}
load(here::here("external/data/label_experiment_metrics.rda"))
consensus <- scores %>% filter(Label == "Consensus") %>% 
  select(AUC, Accuracy, TSS) 
high <- scores %>% filter(Label == "High") %>% select(AUC, Accuracy, TSS) 
low <- scores %>% filter(Label == "Low") %>% select(AUC, Accuracy, TSS)
auc <- high$AUC / low$AUC - 1
tss <- (high$TSS + 1) / (low$TSS + 1) - 1
acc <- high$Accuracy / low$Accuracy - 1
cauc <- consensus$AUC / high$AUC - 1
ctss <- (consensus$TSS + 1) / (high$TSS + 1) - 1
cacc <- consensus$Accuracy / high$Accuracy - 1
```

We used the validation labels to evaluate the accuracy of maps generated in three AOIs (1, 8, and 15; **Figure X**) with the least and most accurate workers' labels, and then compared the resulting scores to accuracy assessed using the consensus labels (**Figure 6**). On average, there was a substantial difference between maps trained with the most and least accurate workers' labels, with the former being on average `r round(mean(acc) * 100, 1)` percent higher (range `r round(min(acc) * 100, 1)`-`r round(max(acc) * 100, 1)`%) in terms of accuracy, `r round(mean(tss) * 100, 1)` percent higher (range `r round(min(tss) * 100, 1)`-`r round(max(tss) * 100, 1)`%) in skill (measured with TSS, which ranges from -1 to 1). Maps generated with consensus labels were on average `r round(mean(cacc) * 100, 1)` percent more accurate (range `r round(min(cacc) * 100, 1)`-`r round(max(cacc) * 100, 1)`%) and `r round(mean(ctss) * 100, 1)` (range `r round(min(ctss) * 100, 1)`-`r round(max(ctss) * 100, 1)`% more skillful. AUC scores showed similar average differences to TSS with respect to both comparisons; on average `r round(mean(auc) * 100, 1)` higher when comparing maps generated with the most and least accurate labels, and `r round(mean(cauc) * 100, 1)` percent more skillful when comparing maps genereated with consensus labels compared to those with the most accurate worker's labels. Although these comparisons were assessed against consensus-generated validation labels, a visual assessment of probability images generated from each labelling strategy shows markedly greater uncertainty between consensus and high accuracy label-generated maps relative to those generated with low accuracy (**see Figure SX in SI**). 


```{r, echo = FALSE, out.width="100%", fig.cap="Difference in overall, user's, and producer's accuracy between maps made with consensus labels, the least accurate worker's, and most accurate worker's labels.", fig.align='center', message=FALSE}
knitr::include_graphics('figures/figure6.png')
# library(tidyverse)
# library(cowplot)
# set.seed(1)
# acc_tb <- tibble(aoi = c(1:9, 12, 15), 
#                  Consensus = runif(11, min = 0.75, max = 0.85),
#                  Highest = runif(11, min = 0.72, max = 0.8),
#                  Least = runif(11, min = 0.65, max = 0.75))
# acc_tb <- gather(acc_tb, key = "type", value = "score", -aoi)
# ggplot(acc_tb) + 
#   geom_boxplot(aes(x = type, y = score), fill = "grey") + 
#   xlab("Labelling strategy") + ylab("Accuracy") + 
#   theme(panel.grid.major.y = element_line(colour = "grey80", linetype = 3))
```

To further evaluate label quality, we assessed the degree of similarity between worker polygons with the field boundaries collected by our ground validation team. Overall, we found that on average a worker's field boundary interpreted from a PlanetScope image had only an **X%** agreement (as determined by area of intersection over union) with the corresponding field-mapped boundary, and the average field size mapped by a worker was on average **X% smaller/larger** than those mapped on the ground. **However, these discrepancies are not necessarily an indication of how far workers were from the "truth", but may instead be at least partially an indication of how different the perspectives are as to what constitutes a field when looking at an image versus seeing it from the ground (which in many cases was based on the field team's judgement, rather than farmer guidance).** 


### Is active learning any better than random? 
To evaluate whether active learning is more effective than a purely random approach to training site selection, we retrained the RandomForest model on the same three AOIs (1, 8, 15) using sites that were selected purely at random for all iterations after the initial training draw. After the mapping team labelled these sites, we subsequently retrained the models with each increment of randomly selected sites, and evaluated prediction performance against the original validation set. 

- Labeling results/quality
- Segmentation results 
  - field size class distributions
  - Improvement of accuracy

# Discussion

- Left unquantified is how much training on base maps matters
- Image quality:
    - why mean-weighting not median composite (artifacts) (maybe methods)
    - impact on accuracy
    
- Assessment of segmentation quality
- Relevant literature: 
    - [@Tongforgottenlanduse2020] discuss Sentinel-based mapping in Sahel, to detect fallows (reference to our intermediate probability values). Semi-automated approach to generating reference data from imagery (based on phenology)
    - [@BeyMappingsmallholderlargescale2020] use CollectEarth to train an algorithm. Multi-stage. No reporting of training or reference data accuracy. 
    - [@LuoDevelopingclearskycloud2008] relevant compositing method to cite, which sounds similar to ours. 

# Acknowledgements


# References
\singlespace

<!-- ```{r write_citations, cache=FALSE, include=FALSE} -->
<!-- write.bibtex(file="knitcitations.bib") -->
<!-- ``` -->

<div id = "refs"></div>


\eleft

\clearpage


<!-- \listoftables -->


\newpage

<!-- ```{r Table1, results='asis', echo=FALSE, cache=FALSE} -->
<!-- kable(head(iris), caption = "A glimpse of the famous *Iris* dataset.") -->
<!-- ``` -->


\newpage

<!-- ```{r Table2, results='asis', echo=FALSE, cache=FALSE} -->
<!-- kable(mtcars[10:16, ], caption = "Now a subset of mtcars dataset.") -->
<!-- ``` -->


\clearpage

<!-- \listoffigures -->


\newpage

<!-- ```{r Fig1, echo=FALSE, fig.cap="Just my first figure with a very fantastic caption.", cache=FALSE} -->
<!-- x <- rnorm(100) -->
<!-- y <- jitter(x, 1000) -->
<!-- plot(x, y) -->
<!-- ``` -->

\newpage

\blandscape

<!-- ```{r Fig2, echo=FALSE, fig.cap="Second figure in landscape format.", cache=FALSE} -->
<!-- a <- sort(rnorm(100)) -->
<!-- b <- c(rep("Group Small", 35), rep("Group Big", 65)) -->
<!-- boxplot(a ~ b) -->
<!-- ``` -->

\elandscape

\clearpage



```{r sessioninfo, echo = FALSE, eval = FALSE}
# set eval = FALSE if you don't want this info (useful for reproducibility) to appear 
sessionInfo()
```

