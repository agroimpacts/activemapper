---
title: "Improving cropland maps through tight integration of human and machine intelligence"


author:  

- name: \*Provisional author list\*
  affilnum: '1'

- name: Lyndon D. Estes
  affilnum: '1'
  email: lestes@clarku.edu

- name: Su Ye
  affilnum: '1'

- name: Lei Song
  affilnum: '1'

- name: Ron Eastman
  affilnum: '1'
  
- name: Sitian Xiong
  affilnum: '1'

- name: Tammy Woodard
  affilnum: '1'

- name: Boka Luo
  affilnum: '1'

- name: Dennis McRitchie
  affilnum: '2'

- name: Ryan Avery
  affilnum: '3'

- name: Kelly Caylor
  affilnum: '3'

- name: Stephanie Debats
  affilnum: '4'

- name: SpatialCollective
  affilnum: '5'

- name: Meridia
  affilnum: '6'

- name: Azavea
  affilnum: '7'

affiliation:

- affilnum: 1
  affil: Graduate School of Geography, Clark University, 950 Main Street, Worcester, MA 01610 USA

- affilnum: 2
  affil: Dennis and Sons, Tucson, AZ, USA

- affilnum: 3
  affil: UCSB

- affilnum: 4
  affil: Uber

- affilnum: 5
  affil: SpatialCollective

- affilnum: 5
  affil: Meridia

- affilnum: 5
  affil: Azavea

output:

  pdf_document:
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_tex: yes
    number_sections: no
    template: manuscript.latex
    includes:
      in_header: header.tex

  html_document: null
  
  word_document: null

documentclass: article
classoption: a4paper
capsize: normalsize
fontsize: 11pt
geometry: margin=1in
linenumbers: yes
spacing: doublespacing
footerdate: yes
abstract: False
bibliography: 
  - references.bib
  - knitcitations.bib
csl: ecology.csl

---


```{r setup, include=FALSE, cache=FALSE, message = FALSE}

library(knitr)
library(citr)

#opts_knit$set(root.dir=normalizePath('../'))

### Chunk options: see http://yihui.name/knitr/options/ ###

## Text results
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)

## Code decoration
opts_chunk$set(tidy = TRUE, comment = NA, highlight = TRUE)

## Cache
opts_chunk$set(cache = 2, cache.path = "output/cache/")

## Plots
opts_chunk$set(fig.path = "output/figures/")


```


```{r knitcitations, echo=FALSE, cache = FALSE}
library(knitcitations)
cleanbib()   
cite_options(citation_format = "pandoc")
```


\singlespace

\vspace{2mm}\hrule
\vspace{-20pt}
# Abstract
The abstract can go either here or in the YAML


\vspace{3mm}\hrule

*Keywords*: rmarkdown, reproducible science

\doublespace

\bleft

# Introduction
How much cropland is on the planet, and where is it located? How much more will be needed to meet humanity's rapidly growing food demands? The answer to the first question is unclear, as existing estimates tend to vary widely and cropland maps show substantial spatial disagreements [e.g. @FritzHighlightingcontinueduncertainty2011; @Fritzneedimprovedmaps2013], which makes it difficult to answer the second. The first question can't be clearly answered because cropland estimates rely heavily on remote sensing-derived landcover maps, which can be notoriously high in error, particularly over regions such as Africa [@Esteslargeareaspatiallycontinuous2018; @FritzComparisonglobalregional2010], where agricultural changes will be largest and the need for an accurate baseline is thus greatest [@EstesReconcilingagriculturecarbon2016; @SearchingerHighcarbonbiodiversity2015]. *[Fill in more reason why cropland maps are important]*  

<!-- - The problem in general  -->
<!--     - Africa in particular -->
<!--     - Types of analyses that depend on cropland maps -->

Cropland mapping over a region such as Africa is difficult for several reasons. The primary reason relates to the characteristics of characteristics of the region's smallholder-dominated croplands, where field size averages 1-2 ha [@Debatsgeneralizedcomputervision2016; @FritzMappingglobalcropland2015]. This size is small relative to the 30-250 m resolution of the sensors typically used for most landcover mapping efforts, which results in errors due to mixed pixels and aspects of the modifiable area unit problem [@Openshawmillioncorrelationcoefficients1979], in this case the pixel's shape may be poorly matched to that of cropland, and is too coarse to aggregate to approximate that shape at the characteristic scales of crop fields [@Darkmodifiablearealunit2007; @Esteslargeareaspatiallycontinuous2018]. On top of the matter of scale is 1) high intra-class variability of the cropland class, compounded by the fact that these particular croplands can be heavily intergraded with surrounding vegetation [@Debatsgeneralizedcomputervision2016; @Estesplatformcrowdsourcingcreation2016], and 2) the substantial temporal variability within croplands, both within and between seasons. These latter two aspects pose challenges for the classification algorithms that are applied to the imagery.  

These problems arising from cropland characteristics are increasingly being overcome due to technological advances. Recent advances in satellite technology have increased the coverage of high (<5 m) or near-high (10 m) resolution imagery with weekly to near-daily return intervals [@DruschSentinel2ESAOptical2012; @McCabefutureEarthobservation2017]. This spatial and temporal resolution addresses the sensor-scale mismatch, and more effectively captures the intra-seasonal dynamics of cropland, which helps classifiers distinguish cropland from surrounding cover types [@Debatsgeneralizedcomputervision2016; @Defournyrealtimeagriculturemonitoring2019]. On top of this, advances in cloud computing [@GorelickGoogleEarthEngine2017a], the opening of image archives [@WulderglobalLandsatarchive2016], and next generation machine learning approaches [@MaxwellImplementationmachinelearningclassification2018; @ZhuDeeplearningremote2017] are placing large volumes of these moderate to near-high resolution imagery together with the computational and algorithmic resources necessary to classify them at scale. These capabilities are aleady being used to create a new generation of higher resolution (20-30 m) cropland and landcover maps for Africa and other regions [@ChenGloballandcover2015; @ESAESACCILAND; @LesivEvaluationESACCI2017; @XiongNominal30mCropland2017].  

Despite these advances, the highest resolution (<5 m) image sources are still not used to map cropland over very large extents, presumably because they are commercial and relatively high cost to acquire, in addition to the greater computational demands. As such, map accuracy can still be a challenge, particular for user's accuracy, which ranges between 46 and 76% for for the cropland class [e.g. @LesivEvaluationESACCI2017; @XiongNominal30mCropland2017]. 

Accuracy may also suffer due to error-inducing factors that are becoming somewhat more pronounced as a consequence of these technology advances, particularly with respect to algorithms. Advances in machine learning are helping to greatly improve classification skill, but these algorithms generally require large training datasets [@MaxwellImplementationmachinelearningclassification2018], particularly neural network-based "deep-learning" methods (cite). To satisfy this need for more training (and reference) samples, map-makers increasingly rely on visual interpretation of high resolution satellite or aerial imagery to collect training (or validation) samples [e.g. @XiongNominal30mCropland2017; @ChenGloballandcover2015; @StehmanKeyissuesrigorous2019]. Several web-based platforms have been developed to facilitate such efforts, which provide convenient and highly scalable tools for training data collection [e.g. @BeyCollectEarthLand2016; @Estesplatformcrowdsourcingcreation2016; @FritzGeoWikionlineplatform2012]. Visually interpreted training labels present two particular problems. The first is that such labels have inevitable interpretation errors that can vary substantially according to the skill of the interpreter [@Estesplatformcrowdsourcingcreation2016; @WaldnerConflationexpertcrowd2019]. These errors are typically not accounted for in reported accuracy metrics (cite Arthur's paper), thus the degree to which such labelling error impacts map accuracy is unclear. However, a study of how land cover map error propagates through subsequent analyses [@Esteslargeareaspatiallycontinuous2018] suggests training data quality could have a large impact. The second problem is that visual interpretation depends on high resolution imagery (<5 m), as lower resolutions make it difficult for a human analyst to discern cropland. Typically the only practical source for such imagery are "virtual globe" basemaps provided by Microsoft and Google, which are composed of mosaics of various high resolution satellite and aerial image sources that typically span 3-5 years of time within a single country [@LesivCharacterizingspatialtemporal2018]. This within-mosaic temporal variation can set up a temporal mismatch between the imagery being interpreted and the imagery being classified, which are usually from a different source (e.g. Landsat, Sentinel; @XiongNominal30mCropland2017) and often represent a different year. If a land change occurs in the interval between the two image sets (e.g. a new field was created), this can introduce error into the training data that is then passed on to the classifier. This source of error may be elevated in smallholder-dominated systems, where swidden practices are common (cite), or in rapidly developing agricultural frontiers [@ZengHighlandcroplandexpansion2018].  
<!-- - Why cropland mapping is hard -->
<!--     - 3 problems -->
<!--         - The target itself and its spectral variability and the spectral variability of its background -->
<!--             - Spectral and spatial resolution of sensors -->
<!--             - Poses a challenge for classification algorithms, which want both high resolution to be smaller than characteristic field size, but temporal contrast also -->


To improve the accuracy of cropland maps over smallholder-dominated systems therefore requires an approach that meets three requirements. First, it should be based on high spatial and temporal resolution imagery, to be able to capture the fine grain and temporal variability of these fields. Second, an algorithm with suitable skill for classifying these images must be selected, and combined with the computational resources needed to process large imagery volumes. Third, a method for collecting large volumes of high quality training and validation data based on image interpretation is essential. This method should quantify and minimize the errors associated with image interpretation. It should also ensure that labels are collected either from the same imagery that is being classified, or from contemporanous imagery, in order to reduce errors introduced by land change processes. 

<!-- - Training data dependence issues -->
<!--     - Also interact with image resolution issues -->
<!--     - Increasingly rely on high amounts of training data -->
<!--     - Human interpretation error (training data quality) -->
<!--       - Temporal mismatch with classification feedstock (Lesiv paper) -->
<!--       - Errors in interpreting these sources of imagery -->
<!-- - Review of who does what to solve the problem (probably integrated with problem statement) -->
<!--   - croplands.org -->


We describe here a cropland mapping system that addresses these requirements on the way to creating an object-based map of individual fields The first requirement is enabled by the recent availability of cubesat data that provides 3-4 m resolution imagery over large areas at near daily intervals [@McCabefutureEarthobservation2017]. Although these data are of lower spectral depth and quality than Landsat, Sentinel, or Worldview imagery, they enable country-continent scale image mosaics to be created for multiple periods during the crop growing calendars, and this representation of intra-annual variability is more important for cropland classification than spectral depth [@Debatsgeneralizedcomputervision2016]. Additionally, despite this imagery being up to ~16 times coarser than much of Bing or Google imagery, it is sufficiently resolved for humans to discern most fields [e.g. see @Esteslargeareaspatiallycontinuous2018; @FourieBetterCropEstimates2009]. This allows labels to be generated on the same imagery processed by the classifier, thereby addressing one of the two needs related to training data (requirement 3). The second requirement is addressed by a computer vision/machine learning classifer that is effective for classifying smallholder croplands [@Debatsgeneralizedcomputervision2016] and was re-engineered to run on high performance, cloud-based computing clusters. This classifier is tightly coupled to a front-end platform for collecting label data, which includes rigorous accuracy assessment protocols [@Estesplatformcrowdsourcingcreation2016] and a novel approach for merging multiple maps into consensus labels, thereby minimizing image interpretation error. The training and machine learning components are combined within an "active learning" framework, wherein the machine learning process assesses classification uncertainty in unlabelled areas after a training step, and selects sites from areas of highest uncertainty for additional labelling [@DebatsIntegratingactivelearning2017]. Our framework automates and formalizes this interactive approach to label selection, which has been shown to be important for improving accuracy in previous studies [e.g. @XiongNominal30mCropland2017; others add]. Furthermore, it is more efficient than purely randomized or conventionally stratified approaches to label collection, meaning that fewer labels are needed to generate a given level of accuracy [@DebatsIntegratingactivelearning2017]. Finally, an unsupervised  segmentation step is applied to the imagery and merged with the pixel-wise classifications from the machine learning process, **resulting in a further sharpening of accuracy** and resulting in a final vector-based field boundary map.

We demonstrate this approach to map cropland in Ghana, a country where agriculture is predominantly small-scale and comprises a broad mix of agricultural systems, including large areas where shifting agricultural practices predominate [@KansangaTraditionalagriculturetransition2019; @SambergSubnationaldistributionaverage2016]. These factors make Ghana emblematic of the substantial challenges facing cropland classification over the broader region. 

<!-- - Our approach that tries to solve these three problems -->
<!-- - Within a framework that tightly integrates tightly training data collection -->
<!-- - And handles large-scale computation -->
<!-- - Segmented boundaries -->
    
# Methods
## System overview 
The mapping system is centered around four major components: 1) an image acquisition and pre-processing component, 2) a training data collection platform, 3) a machine learning component, and a 4) segmentation component. The first component is applied as a once-off step to generate an image catalog necessary for the mapping geography, while the second component initiates the the interactive image training and classification (component 3) pipeline that results in a pixel-wise posterior probability map of cropland probabilities on its final iteration (Fig. 1). The final segmentation step (component 4) is then initiated and applied to both the pre-processed primary images and the posterior probability maps, resulting in the final vectorized fields boundary maps.

## System components
### Imagery
The primary image source used in our system is PlanetScope Analytic surface reflectance product [@PlanetTeamPlanetApplicationProgram2017], although can be readily adapted to apply to Sentinel and Landsat images. PlanetScope provides three visual (red, green, blue) and a near-infrared band at 3-4 m resolution at nominal daily frequency. Although these images are substantially pre-preprocessed and corrected for atmospheric effects, there are substantial residual errors from inter-sensor differences and the radiometric normalization process [@HouborgDailyRetrievalNDVI2018], substantial variation in the orientation of scene footprints, as well as a high frequency of cloud cover over the study region [@WilsonRemotelySensedHighResolution2016]. To correct for these factors, we developed a procedure for creating temporal composites representing the primary growing and non-growing seasons within a single year.  

```{r, echo = FALSE, out.width="50%", fig.cap="An overview of the primary system components, the data stores that hold the inputs and outputs from each component, and the direction of connections between them. The dashed line indicates iterative interactions, while solid lines indicate one-time or irregular connections.", fig.align='center'}
knitr::include_graphics('figures/figure1.png')
```

PlanetScope imagery is accessed via the Planet API, and an initial order is placed wherein the imagery representing all available dates in the two compositing periods and within a bounding box covering the mapping geography is collected and transferred directly to the system's cloud storage (hosted on Amazon Web Services S3). Imagery that don't having certain quality flags are excluded (e.g. failing to meet minimum spatial precision).  

We then transform all images into an "analysis ready data" (ARD) product (cite) that is prepared against a reference grid that is situated within a larger 1 degree resolution grid that covers the entire continent. Each 1 degree cell intersecting the mapping geography (Fig. 2A) is divided into 400 0.05 degree cells (Fig. 2B), which are the reference cells for image compositing. The ARD process then clips the imagery intersecting each 0.05 cell. 
<!-- and then applies the usable data mask (UDM) supplied with the imagery to remove low quality and cloudy pixels.  -->

```{r, echo = FALSE, out.width="100%", fig.cap="Figure scales", fig.align='center'}
knitr::include_graphics('figures/figure2.png')
```
The temporal compositing process is then applied to the ARD imagery, and is done for two time periods. In Ghana, a composite is made to cover the dry season period (December-February) and to span the entire wet season (May - September).  The longer period is necessary for the wet season because of the high frequency of cloud cover, which severely limits the number of clear scenes over any given area. For each image in the compositing process, we pass a 3X3 median filter over the image to remove speckling, and then calculate a weight value based on a simplified formula of the Haze Optimized Transformation (HOT) index [@Zhangimagetransformcharacterize2002; @ZhuObjectbasedcloudcloud2012]: 

$$
HOT_{weight} = \frac{1}{(blue - 0.5red)^2}
$$
This assigns a low weight to pixels that are cloudy, hazy, or smoky pixels on a particular date. We then use the weights to calculate a weight mean for each pixel and each band for the final composite for each time period. The composites are then added to the S3 store (Fig. 1), and a "slippy map" rendering is created for each composite using RasterFoundry (cite). This web-based rendering of the image is used in the training data platform (see section X). 

### Training data platform
- Training and validation data
- Accuracy assessment

### Classification pipeline
- Feature engineering
- ML pipeline Apache Spark

### Segmentation

## The active learning loop


# Results

- Image quality and density
- Computational time/resources, including interaction time
- Labeling results/quality
- Impact of features on accuracy
- Accuracy scores by iteration
- Accuracy by label quality
- Segmentation results 
  - field size class distributions
  - Improvement of accuracy

# Discussion

- Left unquantified is how much training on base maps matters

# Acknowledgements


# References
\singlespace

<!-- ```{r write_citations, cache=FALSE, include=FALSE} -->
<!-- write.bibtex(file="knitcitations.bib") -->
<!-- ``` -->

<div id = "refs"></div>


\eleft

\clearpage


<!-- \listoftables -->


\newpage

<!-- ```{r Table1, results='asis', echo=FALSE, cache=FALSE} -->
<!-- kable(head(iris), caption = "A glimpse of the famous *Iris* dataset.") -->
<!-- ``` -->


\newpage

<!-- ```{r Table2, results='asis', echo=FALSE, cache=FALSE} -->
<!-- kable(mtcars[10:16, ], caption = "Now a subset of mtcars dataset.") -->
<!-- ``` -->


\clearpage

<!-- \listoffigures -->


\newpage

<!-- ```{r Fig1, echo=FALSE, fig.cap="Just my first figure with a very fantastic caption.", cache=FALSE} -->
<!-- x <- rnorm(100) -->
<!-- y <- jitter(x, 1000) -->
<!-- plot(x, y) -->
<!-- ``` -->

\newpage

\blandscape

<!-- ```{r Fig2, echo=FALSE, fig.cap="Second figure in landscape format.", cache=FALSE} -->
<!-- a <- sort(rnorm(100)) -->
<!-- b <- c(rep("Group Small", 35), rep("Group Big", 65)) -->
<!-- boxplot(a ~ b) -->
<!-- ``` -->

\elandscape

\clearpage



```{r sessioninfo, echo = FALSE, eval = FALSE}
# set eval = FALSE if you don't want this info (useful for reproducibility) to appear 
sessionInfo()
```

