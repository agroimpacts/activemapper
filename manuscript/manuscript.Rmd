---
title: "Improving cropland maps through tight integration of human and machine intelligence"


author:  

- name: \*Provisional author list\*
  affilnum: '1'

- name: Lyndon D. Estes
  affilnum: '1'
  email: lestes@clarku.edu

- name: Su Ye
  affilnum: '1'

- name: Lei Song
  affilnum: '1'

- name: Ron Eastman
  affilnum: '1'
  
- name: Sitian Xiong
  affilnum: '1'

- name: Tammy Woodard
  affilnum: '1'

- name: Boka Luo
  affilnum: '1'

- name: Dennis McRitchie
  affilnum: '2'

- name: Ryan Avery
  affilnum: '3'

- name: Kelly Caylor
  affilnum: '3'

- name: Stephanie Debats
  affilnum: '4'

- name: SpatialCollective
  affilnum: '5'

- name: Meridia
  affilnum: '6'

- name: Azavea
  affilnum: '7'

affiliation:

- affilnum: 1
  affil: Graduate School of Geography, Clark University, 950 Main Street, Worcester, MA 01610 USA

- affilnum: 2
  affil: Dennis and Sons, Tucson, AZ, USA

- affilnum: 3
  affil: UCSB

- affilnum: 4
  affil: Uber

- affilnum: 5
  affil: SpatialCollective

- affilnum: 5
  affil: Meridia

- affilnum: 5
  affil: Azavea

output:

  pdf_document:
    fig_caption: yes
    fig_width: 7
    fig_height: 7
    keep_tex: yes
    number_sections: yes
    template: manuscript.latex
    includes:
      in_header: header.tex

  html_document: null
  
  word_document: null

documentclass: article
classoption: a4paper
capsize: normalsize
fontsize: 11pt
geometry: margin=1in
linenumbers: yes
spacing: doublespacing
footerdate: yes
abstract: The abstract can go either here or below
keywords: rmarkdown, reproducible science
bibliography: 
  - references.bib
  - knitcitations.bib
csl: ecology.csl

---


```{r setup, include=FALSE, cache=FALSE, message = FALSE}

library(knitr)
library(citr)

#opts_knit$set(root.dir=normalizePath('../'))

### Chunk options: see http://yihui.name/knitr/options/ ###

## Text results
opts_chunk$set(echo = TRUE, warning = TRUE, message = TRUE, include = TRUE)

## Code decoration
opts_chunk$set(tidy = TRUE, comment = NA, highlight = TRUE)

## Cache
opts_chunk$set(cache = 2, cache.path = "output/cache/")

## Plots
opts_chunk$set(fig.path = "output/figures/")


```


```{r knitcitations, echo=FALSE, cache = FALSE}
library(knitcitations)
cleanbib()   
cite_options(citation_format = "pandoc")
```


<!-- \singlespace -->

<!-- \vspace{2mm}\hrule -->
<!-- \vspace{-20pt} -->
<!-- <!-- # Abstract --> 


<!-- \vspace{3mm}\hrule -->

<!-- *Keywords*: rmarkdown, reproducible science -->

\doublespace

\bleft

# Introduction
How much cropland is on the planet, and where is it located? How much more will be needed to meet humanity's rapidly growing food demands? The answer to the first question is unclear, as existing estimates tend to vary widely and cropland maps show substantial spatial disagreements [e.g. @FritzHighlightingcontinueduncertainty2011; @Fritzneedimprovedmaps2013], which makes it difficult to answer the second. The first question can't be clearly answered because cropland estimates rely heavily on remote sensing-derived landcover maps, which can be notoriously high in error, particularly over regions such as Africa [@Esteslargeareaspatiallycontinuous2018; @FritzComparisonglobalregional2010], where agricultural changes will be largest and the need for an accurate baseline is thus greatest [@EstesReconcilingagriculturecarbon2016; @SearchingerHighcarbonbiodiversity2015]. *[Fill in more reason why cropland maps are important]*  

<!-- - The problem in general  -->
<!--     - Africa in particular -->
<!--     - Types of analyses that depend on cropland maps -->

Cropland mapping over a region such as Africa is difficult for several reasons. The primary reason relates to the characteristics of characteristics of the region's smallholder-dominated croplands, where field size averages 1-2 ha [@Debatsgeneralizedcomputervision2016; @FritzMappingglobalcropland2015]. This size is small relative to the 30-250 m resolution of the sensors typically used for most landcover mapping efforts, which results in errors due to mixed pixels and aspects of the modifiable area unit problem [@Openshawmillioncorrelationcoefficients1979], in this case the pixel's shape may be poorly matched to that of cropland, and is too coarse to aggregate to approximate that shape at the characteristic scales of crop fields [@Darkmodifiablearealunit2007; @Esteslargeareaspatiallycontinuous2018]. On top of the matter of scale is 1) high intra-class variability of the cropland class, compounded by the fact that these particular croplands can be heavily intergraded with surrounding vegetation [@Debatsgeneralizedcomputervision2016; @Estesplatformcrowdsourcingcreation2016], and 2) the substantial temporal variability within croplands, both within and between seasons. These latter two aspects pose challenges for the classification algorithms that are applied to the imagery.  

These problems arising from cropland characteristics are increasingly being overcome due to technological advances. Recent advances in satellite technology have increased the coverage of high (<5 m) or near-high (10 m) resolution imagery with weekly to near-daily return intervals [@DruschSentinel2ESAOptical2012; @McCabefutureEarthobservation2017]. This spatial and temporal resolution addresses the sensor-scale mismatch, and more effectively captures the intra-seasonal dynamics of cropland, which helps classifiers distinguish cropland from surrounding cover types [@Debatsgeneralizedcomputervision2016; @Defournyrealtimeagriculturemonitoring2019]. On top of this, advances in cloud computing [@GorelickGoogleEarthEngine2017a], the opening of image archives [@WulderglobalLandsatarchive2016], and next generation machine learning approaches [@MaxwellImplementationmachinelearningclassification2018; @ZhuDeeplearningremote2017] are placing large volumes of these moderate to near-high resolution imagery together with the computational and algorithmic resources necessary to classify them at scale. These capabilities are aleady being used to create a new generation of higher resolution (20-30 m) cropland and landcover maps for Africa and other regions [@ChenGloballandcover2015; @ESAESACCILAND; @LesivEvaluationESACCI2017; @XiongNominal30mCropland2017].  

Despite these advances, the highest resolution (<5 m) image sources are still not used to map cropland over very large extents, presumably because they are commercial and relatively high cost to acquire, in addition to the greater computational demands. As such, map accuracy can still be a challenge, particular for user's accuracy, which ranges between 46 and 76% for for the cropland class [e.g. @LesivEvaluationESACCI2017; @XiongNominal30mCropland2017]. 

Accuracy may also suffer due to error-inducing factors that are becoming somewhat more pronounced as a consequence of these technology advances, particularly with respect to algorithms. Advances in machine learning are helping to greatly improve classification skill, but these algorithms generally require large training datasets [@MaxwellImplementationmachinelearningclassification2018], particularly neural network-based "deep-learning" methods (cite). To satisfy this need for more training (and reference) samples, map-makers increasingly rely on visual interpretation of high resolution satellite or aerial imagery to collect training (or validation) samples [e.g. @XiongNominal30mCropland2017; @ChenGloballandcover2015; @StehmanKeyissuesrigorous2019]. Several web-based platforms have been developed to facilitate such efforts, which provide convenient and highly scalable tools for training data collection [e.g. @BeyCollectEarthLand2016; @Estesplatformcrowdsourcingcreation2016; @FritzGeoWikionlineplatform2012]. Visually interpreted training labels present two particular problems. The first is that such labels have inevitable interpretation errors that can vary substantially according to the skill of the interpreter [@Estesplatformcrowdsourcingcreation2016; @WaldnerConflationexpertcrowd2019]. These errors are typically not accounted for in reported accuracy metrics (cite Arthur's paper), thus the degree to which such labelling error impacts map accuracy is unclear. However, a study of how land cover map error propagates through subsequent analyses [@Esteslargeareaspatiallycontinuous2018] suggests training data quality could have a large impact. The second problem is that visual interpretation depends on high resolution imagery (<5 m), as lower resolutions make it difficult for a human analyst to discern cropland. Typically the only practical source for such imagery are "virtual globe" basemaps provided by Microsoft and Google, which are composed of mosaics of various high resolution satellite and aerial image sources that typically span 3-5 years of time within a single country [@LesivCharacterizingspatialtemporal2018]. This within-mosaic temporal variation can set up a temporal mismatch between the imagery being interpreted and the imagery being classified, which are usually from a different source (e.g. Landsat, Sentinel; @XiongNominal30mCropland2017) and often represent a different year. If a land change occurs in the interval between the two image sets (e.g. a new field was created), this can introduce error into the training data that is then passed on to the classifier. This source of error may be elevated in smallholder-dominated systems, where swidden practices are common (cite), or in rapidly developing agricultural frontiers [@ZengHighlandcroplandexpansion2018].  
<!-- - Why cropland mapping is hard -->
<!--     - 3 problems -->
<!--         - The target itself and its spectral variability and the spectral variability of its background -->
<!--             - Spectral and spatial resolution of sensors -->
<!--             - Poses a challenge for classification algorithms, which want both high resolution to be smaller than characteristic field size, but temporal contrast also -->


To improve the accuracy of cropland maps over smallholder-dominated systems therefore requires an approach that meets three requirements. First, it should be based on high spatial and temporal resolution imagery, to be able to capture the fine grain and temporal variability of these fields. Second, an algorithm with suitable skill for classifying these images must be selected, and combined with the computational resources needed to process large imagery volumes. Third, a method for collecting large volumes of high quality training and validation data based on image interpretation is essential. This method should quantify and minimize the errors associated with image interpretation. It should also ensure that labels are collected either from the same imagery that is being classified, or from contemporanous imagery, in order to reduce errors introduced by land change processes. 

<!-- - Training data dependence issues -->
<!--     - Also interact with image resolution issues -->
<!--     - Increasingly rely on high amounts of training data -->
<!--     - Human interpretation error (training data quality) -->
<!--       - Temporal mismatch with classification feedstock (Lesiv paper) -->
<!--       - Errors in interpreting these sources of imagery -->
<!-- - Review of who does what to solve the problem (probably integrated with problem statement) -->
<!--   - croplands.org -->


We describe here a cropland mapping system that addresses these requirements on the way to creating an object-based map of individual fields The first requirement is enabled by the recent availability of cubesat data that provides 3-4 m resolution imagery over large areas at near daily intervals [@McCabefutureEarthobservation2017]. Although these data are of lower spectral depth and quality than Landsat, Sentinel, or Worldview imagery, they enable country-continent scale image mosaics to be created for multiple periods during the crop growing calendars, and this representation of intra-annual variability is more important for cropland classification than spectral depth [@Debatsgeneralizedcomputervision2016]. Additionally, despite this imagery being up to ~16 times coarser than much of Bing or Google imagery, it is sufficiently resolved for humans to discern most fields [e.g. see @Esteslargeareaspatiallycontinuous2018; @FourieBetterCropEstimates2009]. This allows labels to be generated on the same imagery processed by the classifier, thereby addressing one of the two needs related to training data (requirement 3). The second requirement is addressed by a computer vision/machine learning classifer that is effective for classifying smallholder croplands [@Debatsgeneralizedcomputervision2016] and was re-engineered to run on high performance, cloud-based computing clusters. This classifier is tightly coupled to a front-end platform for collecting label data, which includes rigorous accuracy assessment protocols [@Estesplatformcrowdsourcingcreation2016] and a novel approach for merging multiple maps into consensus labels, thereby minimizing image interpretation error. The training and machine learning components are combined within an "active learning" framework, wherein the machine learning process assesses classification uncertainty in unlabelled areas after a training step, and selects sites from areas of highest uncertainty for additional labelling [@DebatsIntegratingactivelearning2017]. Our framework automates and formalizes this interactive approach to label selection, which has been shown to be important for improving accuracy in previous studies [e.g. @XiongNominal30mCropland2017; others add]. Furthermore, it is more efficient than purely randomized or conventionally stratified approaches to label collection, meaning that fewer labels are needed to generate a given level of accuracy [@DebatsIntegratingactivelearning2017]. Finally, an unsupervised  segmentation step is applied to the imagery and merged with the pixel-wise classifications from the machine learning process, **resulting in a further sharpening of accuracy** and resulting in a final vector-based field boundary map.

We demonstrate this approach to map cropland in Ghana, a country where agriculture is predominantly small-scale and comprises a broad mix of agricultural systems, including large areas where shifting agricultural practices predominate [@KansangaTraditionalagriculturetransition2019; @SambergSubnationaldistributionaverage2016]. These factors make Ghana emblematic of the substantial challenges facing cropland classification over the broader region. 

<!-- - Our approach that tries to solve these three problems -->
<!-- - Within a framework that tightly integrates tightly training data collection -->
<!-- - And handles large-scale computation -->
<!-- - Segmented boundaries -->
    
# Methods
## System overview 
The mapping system is centered around four major components: 1) an image acquisition and pre-processing component, 2) a training data collection platform, 3) a machine learning component, and a 4) segmentation component. The first component is applied as a once-off step to generate an image catalog necessary for the mapping geography, while the second component initiates the the interactive image training and classification (component 3) pipeline that results in a pixel-wise posterior probability map of cropland probabilities on its final iteration (Fig. 1). The final segmentation step (component 4) is then initiated and applied to both the pre-processed primary images and the posterior probability maps, resulting in the final vectorized fields boundary maps. Each system component comprises an individual module software hosted on a GitHub repository (see data and software availability section for details). 

## System components
### Imagery
**[Note: This will require updating to reflect changes to compositing methods]**
The primary image source used in our system is PlanetScope Analytic surface reflectance product [@PlanetTeamPlanetApplicationProgram2017], although can be readily adapted to apply to Sentinel and Landsat images. PlanetScope provides three visual (red, green, blue) and a near-infrared band at 3-4 m resolution at nominal daily frequency. Although these images are substantially pre-preprocessed and corrected for atmospheric effects, there are substantial residual errors from inter-sensor differences and the radiometric normalization process [@HouborgDailyRetrievalNDVI2018], substantial variation in the orientation of scene footprints, as well as a high frequency of cloud cover over the study region [@WilsonRemotelySensedHighResolution2016]. To correct for these factors, we developed a procedure for creating temporal composites representing the primary growing and non-growing seasons within a single year.  

```{r, echo = FALSE, out.width="50%", fig.cap="An overview of the primary system components, the data stores that hold the inputs and outputs from each component, and the direction of connections between them. The dashed line indicates iterative interactions, while solid lines indicate one-time or irregular connections.", fig.align='center'}
knitr::include_graphics('figures/figure1.png')
```

PlanetScope imagery is accessed via the Planet API, and an initial order is placed wherein the imagery representing all available dates in the two compositing periods and within a bounding box covering the mapping geography is collected and transferred directly to the system's cloud storage (hosted on Amazon Web Services S3). Imagery that don't having certain quality flags are excluded (e.g. failing to meet minimum spatial precision).  

We then transform all images into an "analysis ready data" (ARD) product (cite) that is prepared against a reference grid that is situated within a larger 1 degree resolution grid that covers the entire continent. Each 1 degree cell defines the minimum mapping area of interest (Fig. 2A) is divided into 400 0.05 degree cells (Fig. 2B), which provides the scale for creating image composite tiles. The ARD process simply clips all available imagery intersecting each 0.05 cell to the extent of that cell, regardless of cloud cover. 

<!-- and then applies the usable data mask (UDM) supplied with the imagery to remove low quality and cloudy pixels.  -->

```{r, echo = FALSE, out.width="100%", fig.cap="An overview of analytical scales used in the mapping platform, including the 1 degree areas of interest defining the minimum mapping geography (A), the 0.05 degree grid used for creating image composite tiles (B), and the 0.005 degree resolution reference grid used for collecting training data and distributed computing (C).", fig.align='center'}
knitr::include_graphics('figures/figure2.png')
```
The temporal compositing process is applied to the ARD imagery for two time periods, the dry season period (December-February) and the entire wet season (May - September). The longer period is necessary for the wet season because of the high frequency of cloud cover, which severely limits the number of clear scenes over any given area. For each pixel in each image in each ARD temporal stack **we pass a 3X3 median filter over the image to remove speckling**, we calculate two weight values. The first is based on a simplified formula of the Haze Optimized Transformation (HOT) index [@Zhangimagetransformcharacterize2002; @ZhuObjectbasedcloudcloud2012]: 

$$
\mathrm{HOT_{weight}} = \frac{1}{(\mathrm{B_1} - 0.5\mathrm{B_3})^2}
$$
Where B$_1$ is the blue band and B$_3$ is the red band. This equation results in low values for pixels that are cloudy, hazy, or smoky pixels on a particular date, and high values for clear images. The second weight is designed to detect cloud shadow: 

$$
\mathrm{Shadow_{weight}} = \frac{1}{(some values)^4}
$$

Where X is the median value across the full time series. This formula assigns a low weight to pixels falling within cloud shadows. Weight values lower than the X for a given date and pixel are assigned to that pixel, replacing the $HOT_{weight}$. We then use the resulting to calculate a weighted mean for each pixel for each band for the final composite for each time period. The composites are then added to the S3 store (Fig. 1), and a "slippy map" rendering is created for each composite using RasterFoundry (cite). This web-based rendering of the image is used in the training data platform (see section X). 


### Labelling platform
Both training and reference data are collected by a custom crowdsourcing platform, which refer to here for simplicity's sake as *mapper*. The original version of *mapper* was designed to work with Amazon Web Services' Mechanical Turk job marketplace [@Estesplatformcrowdsourcingcreation2016]. The basic structure of the system remains the same, but we have re-engineered it to be a standalone platform that 1) allows us to register and pay workers (the human supervisors) directly to conduct digitizing work, and 2) drives and assesses the overall machine learning process. The system runs on a Linux Virtual machine hosted on an AWS EC2 instance and is comprised of a database (PostGIS/Postgres), a mapping interface (OpenLayers 3), an image server (RasterFoundry), and a set of utilities for managing mapping and assessing assignments and converting them to rasterized labels for the machine learning process. Each *mapper* instance is focused on a specific mapping geography (e.g. one to several 1 degree cells in Fig. 2A), thus several instances are set up to run in parallel to cover a larger mapping geography such as Ghana. 

Here we provide an overview of *mapper*'s architecture within the context of the labeling workflow within a single instance.

#### Mapping workflow
##### Selecting training and reference sites
The labelling process begins with a python script that randomly selects a subset (e.g. 100) of the 0.005 degree grid (Fig. 2C) cells that intersect the *mapper* instance's specific area of interest (e.g. shaded grey cell in Fig. 2A). These site names are stored in a Postgres table that represents the mapping queue. Another python process converts each site into a mapping *task* that has a specified number of *assignments* (maps drawn by an individual worker) that must be completed before the task is complete.  


##### Mapping assignments
Workers registered in the system log in to the mapping platform (built with Flask) and navigate to the OpenLayers-based field mapping interface (Fig. 3). There the worker is presented with a white target box representing the 0.005 degree cell that was randomly selected to label, a set of digitizing tools, and different image backdrops, which include false and true color renderings of both the growing season and off-growing season PlanetScope images covering that cell, as well as a basemap provided by Bing maps. The worker uses the polygon drawing tool to digitize the boundaries in crop fields visible within the PlanetScope overlays that intersect the grid target, following a set of pre-defined digitizing rules. To aid with interpretation, the worker toggles between the various seasonal renderings and the satellite base map to help form a judgement about what constitutes a field within the PlanetScope imagery. The worker assigns each completed polygon a label that describes the field class (e.g. annual cropland, tree crop, fallow), and after completing all visible fields saves the full set of polygons to the server. The worker is then automatically  taken to their next assignment at a different randomly selected site. 


```{r, echo = FALSE, out.width="100%", fig.cap="An overview of mapper's interface", fig.align='center'}
knitr::include_graphics('figures/figure3.png')
```

##### Processing completed assignments
Upon submission, the workers' polygons, if there are any--the worker will submit the assignment without mapping if they see no fields in the image--are are cleaned to fix any topological irregularities that arose during digitization (see supporting information (SI)) and stored in a PostGIS table. The assignment that was just completed represents one of two possible types of tasks: 1) an accuracy task; 2) a labeling task for training machine learning. If is was the former type, an `R` script is called that executes a series of map comparisons between the worker's polygons and the reference polygons, assigning a score as based on the following formula:

$$\mathrm{score_i}=\beta0\mathrm{I}+\beta1\mathrm{O}+\beta2\mathrm{F}+\beta3\mathrm{E}+\beta4\mathrm{C}$$

Where *i* represents the particular assignment, and $\beta_{0-5}$ represent varying weights that sum to 1. *I* refers to "inside the box" accuracy, *O* is the accuracy of those portions of the worker's polygons extending beyond the target grid boundaries, *F* is fragmentation accuracy, a measure of how many individual polygons the worker delineated relative to the reference, *E* measures how closely each polygon's boundary matched its corresponding reference polygon boundary, and *C* assesses the accuracy of the worker's thematic labels (see SI for individual formulae).

Over time, workers are assessed multiple times across a range of accuracy tasks that are selected to represent the variability of the agricultural system being mapped. Each worker's score history is averaged to provide an overall assessment of accuracy, and this information is used for creating labels, the second task. 

If the worker's completed assignment was a labelling tasks, their maps remain stored in the database until the remaining assignments needed to complete the task are submitted by other workers. Once complete, an R-based consensus labelling routine is invoked, which uses combines the maps from all workers who undertook the task into a single consensus label using a Bayesian merging approach:

$$
P(\theta|\mathrm{D})=\sum_{i=1}^{n}\mathrm{P}(\mathrm{W_i}|\mathrm{D})\mathrm{P}(\mathrm{\theta}|\mathrm{D}, \mathrm{W_i})
$$
Where $\theta$ represents the true cover type of a pixel (field or not field), *D* is the worker's label of that field, and *W_$i$* is an individual worker.  P($\theta$|D) is therefore the probability that the actual cover type is what the workers who labelled it says it is, while P(W$_i$|D) represents the average producer's accuracy for a given worker, and P(W$\theta$|D, *W$_i$*) is the individual worker's label for that pixel. This approach therefore uses the overall accuracy of each worker to weight their labels as they are combined with other workers' labels for a given pixel (see SI for further details of equation). 







- Training and validation data
- Accuracy assessment

### Classification pipeline
- Feature engineering
- ML pipeline Apache Spark

### Segmentation

## The active learning loop


# Results

- Image quality and density
- Computational time/resources, including interaction time
- Labeling results/quality
- Impact of features on accuracy
- Accuracy scores by iteration
- Accuracy by label quality
- Segmentation results 
  - field size class distributions
  - Improvement of accuracy

# Discussion

- Left unquantified is how much training on base maps matters

# Acknowledgements


# References
\singlespace

<!-- ```{r write_citations, cache=FALSE, include=FALSE} -->
<!-- write.bibtex(file="knitcitations.bib") -->
<!-- ``` -->

<div id = "refs"></div>


\eleft

\clearpage


<!-- \listoftables -->


\newpage

<!-- ```{r Table1, results='asis', echo=FALSE, cache=FALSE} -->
<!-- kable(head(iris), caption = "A glimpse of the famous *Iris* dataset.") -->
<!-- ``` -->


\newpage

<!-- ```{r Table2, results='asis', echo=FALSE, cache=FALSE} -->
<!-- kable(mtcars[10:16, ], caption = "Now a subset of mtcars dataset.") -->
<!-- ``` -->


\clearpage

<!-- \listoffigures -->


\newpage

<!-- ```{r Fig1, echo=FALSE, fig.cap="Just my first figure with a very fantastic caption.", cache=FALSE} -->
<!-- x <- rnorm(100) -->
<!-- y <- jitter(x, 1000) -->
<!-- plot(x, y) -->
<!-- ``` -->

\newpage

\blandscape

<!-- ```{r Fig2, echo=FALSE, fig.cap="Second figure in landscape format.", cache=FALSE} -->
<!-- a <- sort(rnorm(100)) -->
<!-- b <- c(rep("Group Small", 35), rep("Group Big", 65)) -->
<!-- boxplot(a ~ b) -->
<!-- ``` -->

\elandscape

\clearpage



```{r sessioninfo, echo = FALSE, eval = FALSE}
# set eval = FALSE if you don't want this info (useful for reproducibility) to appear 
sessionInfo()
```

